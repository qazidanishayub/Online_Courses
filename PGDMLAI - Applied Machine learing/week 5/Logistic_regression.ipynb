{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "**Author: Khal Makhoul, W.P.G.Peterson**\n",
    "\n",
    "**_Revised: Jessica Cervi_**\n",
    "\n",
    "**Expected time = 2.5 hours**\n",
    "\n",
    "**Total points = 80 points**\n",
    " \n",
    " \n",
    " \n",
    "## Assignment Overview\n",
    "\n",
    "\n",
    "Logistic regression offers a way to create a fairly interpretable parametric model for binary classification. This assignment will work through the definition of a logistic regression function in Python. After summarizing the equations and a brief EDA of the 'Titanic' data that we will be using, you will be asked to define many functions that will create a logistic regression.  \n",
    "A demonstration of sklearn's implementation of Logistic Regression will close the assignment.\n",
    "\n",
    "\n",
    "You will be asked to code functions to do the following:\n",
    "1. Implement the Logistic Regression Algorithm\n",
    "    - Calculate the value of the sigmoid function\n",
    "    - Calculate the gradient of the log-likelihood with respect to $w$\n",
    "    - Sum the gradients of the log-likelihood with respect to $w$\n",
    "2. Execute logistic regression, stopping after a particular iteration\n",
    "3. Determine convergence of the logistic regression algorithm\n",
    "\n",
    "This assignment is designed to build your familiarity and comfort in coding in Python. It will also help you review the key topics from each module. As you progress through the assignment, answers to the questions will get increasingly complex. You must adopt a data scientist's mindset when completing this assignment. **Remember to run your code from each cell before submitting your assignment.** Running your code beforehand will notify you of errors and giving you a chance to fix your errors before submitting. You should view your Vocareum submission as if you are delivering a final project to your manager or client. \n",
    "\n",
    "***Vocareum Tips***\n",
    "- Do not add arguments or options to functions unless asked specifically. This will cause an error in Vocareum.\n",
    "- Do not use a library unless you are explicitly asked in the question. \n",
    "- You can download the Grading Report after submitting the assignment. It will include the feedback and hints on incorrect questions. \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "\n",
    "- Count the null values \n",
    "- Examine the auto coorrelation features \n",
    "- Ensure the correct orientation of the feature matrix \n",
    "- Implement the logistic function \n",
    "- Update the coefficient/weights using the gradient descent  \n",
    "- Examine the predicted probabilities using the logistic regression  \n",
    "- Implement the logistic regression with sklearn\n",
    "\n",
    "\n",
    "\n",
    "## Index: \n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "- [Question 1](#q1)\n",
    "- [Question 2](#q2)\n",
    "- [Question 3](#q3)\n",
    "- [Question 4](#q4)\n",
    "- [Question 5](#q5)\n",
    "- [Question 6](#q6)\n",
    "- [Question 7](#q7)\n",
    "- [Question 8](#q8)\n",
    "- [Question 9](#q9)\n",
    "- [Question 10](#q10)\n",
    "- [Question 11](#q11)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Problem**: Using Logistic Regression, predict whether or not a passenger survived the sinking of the Titanic.\n",
    "\n",
    "**Data**: Today's data comes from [Kaggle's Titanic Data](https://www.kaggle.com/c/titanic/data). Please see the above link for the complete description of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "In this week's lectures, we derived all the equations that we will use in this assignment.  \n",
    "\n",
    "Recall that the likelihood for Logistic Regression is given by:\n",
    "\n",
    "$$p(y_1,\\ ...,\\ y_n\\ |\\ x_1,\\ ...,\\ x_n,\\ w)\\ =\\prod\\limits_{i=1}^n\\ \\sigma_i(y_i \\cdot w)$$  \n",
    "\n",
    "For coding purposes, we need the expression for the gradient of the log-likelihood $\\mathcal{L}$ with respect to $w$:\n",
    "\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\sum_{i = 1}^n (1 âˆ’ \\sigma_i(y_i \\cdot w))\\ y_i x_i$$  \n",
    "\n",
    "Where: $$\\sigma_i(y_i \\cdot w) = \\frac{e^{y_iX_i^Tw}}{1+e^{y_ix_i^Tw}}$$  \n",
    "\n",
    "The steps for implementing the logistic regression is explained at the beginning of each of the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset and exploratory data analysis\n",
    "\n",
    "\n",
    "This assignment will analyze data from [Titanic passenger manifest](https://www.kaggle.com/c/titanic/data). Our goal will be to analyze the demographic and trip information for each passenger to understand whether they have survived or not the disaster.\n",
    "\n",
    "We start by examining the data as usual:\n",
    "Before coding an algorithm, we will take a look at our data using Python's pandas. For visualizations, we will use matplotlib.\n",
    "\n",
    "To import the necessary libraries and load the datasets, we will be using the pandas' 'pd.read_csv()' function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "# Load the data into a `pandas` DataFrame object\n",
    "titanic_df = pd.read_csv('./data/train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by performing some basic exploratory data analysis by using the function `head()` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine head of df\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q1'></a>\n",
    "\n",
    "### Question 1:\n",
    "\n",
    "*5 points*\n",
    " \n",
    "When deciding whether we want to drop a column in a DataFrame, some general rules are as follows:\n",
    "\n",
    "- If a column consists mostly of missing data, that column probably will not be of much use in prediction.  \n",
    "- If a column has a very few missing values, and there enough records to build a model, the records with missing values in that column may be cast out.  \n",
    "\n",
    "Drop all of the columns in 'titanic_df' that are filled with more than 50% nulls. Next, if a column has less than 10 missing values, drop all of the records with missing data in that column.\n",
    "\n",
    "After performing the above operations, what is the shape of the DataFrame? We assign integers to the variables 'row' and 'cols' below corresponding to the *remaining* number of rows/columns in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "shape",
     "locked": false,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "rows = 889\n",
    "cols = 11\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "\n",
    "# print(titanic_df.shape)\n",
    "# titanic_df = titanic_df.drop(['Cabin' ], axis=1)\n",
    "# print(titanic_df.isna().sum())\n",
    "# titanic_df = titanic_df.dropna(how='any', subset=['Embarked'])\n",
    "# titanic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q2'></a>\n",
    "\n",
    "### Question 2:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "How many values are missing from the column \"Age\" ? Assign your answer to the variable \"ans2\" as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans2 = 177\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "\n",
    "#titanic_df['Age'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Given:\n",
    "- The large number of values missing in the column 'Age'\n",
    "- The age of a passenger is likely connected with the chances of survival of the passenger\n",
    "\n",
    "In such a case, we will create an educated guess for the missing missing values of the passengers' ages. In other words, we will impute the ages using the $k$-Nearest-Neighbor (KNN) algorithm.\n",
    "\n",
    "Note: In imputing values for 'Age', we will exclude the feature 'Survived' from the matrix $X$, because we want to predict the chances of survival of each passenger.\n",
    "\n",
    "#### KNeighborsRegressor in \"sklearn\"\n",
    "It is necessary to encode any categorical features that we will be using as dummy variables because 'sklearn' automatically converts all data to floats before fitting the models. \n",
    "\n",
    "We begin by dropping irrelevant features, such as 'Ticket', 'Cabin', 'PassengerId', and 'Name', from the data frame. We also drop the 'Survived' column for the reason we just explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0       3    male  22.0      1      0   7.2500        S\n",
       "1       1  female  38.0      1      0  71.2833        C\n",
       "2       3  female  26.0      0      0   7.9250        S\n",
       "3       1  female  35.0      1      0  53.1000        S\n",
       "4       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.drop(['Ticket','Cabin', 'PassengerId', 'Name'], axis=1, inplace=True)\n",
    "titanic_df = titanic_df.loc[titanic_df['Embarked'].notnull(),:]\n",
    "\n",
    "### Drop \"Survived\" for purposes of KNN imputation:\n",
    "y_target = titanic_df.Survived\n",
    "titanic_knn = titanic_df.drop(['Survived'], axis = 1)  \n",
    "titanic_knn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the function \"get_dummies()\" on the columns \"Sex\" and \"Embarked\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S\n",
       "0       3  22.0      1      0   7.2500         1           0           1\n",
       "1       1  38.0      1      0  71.2833         0           0           0\n",
       "2       3  26.0      0      0   7.9250         0           0           1\n",
       "3       1  35.0      1      0  53.1000         0           0           1\n",
       "4       3  35.0      0      0   8.0500         1           0           1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Adding the dummy variables for categorical variables\n",
    "to_dummy = ['Sex','Embarked']\n",
    "titanic_knn = pd.get_dummies(titanic_knn, prefix = to_dummy, columns = to_dummy, drop_first = True)\n",
    "\n",
    "titanic_knn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will follow the same steps as we did in Assignment 16. We split the data and make a prediction about the missing values using the KNN algorithm.\n",
    "\n",
    "The code is given in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data to Impute\n",
      "    Pclass  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S\n",
      "5        3      0      0   8.4583         1           1           0\n",
      "17       2      0      0  13.0000         1           0           1\n",
      "19       3      0      0   7.2250         0           0           0\n",
      "\n",
      "Imputed Ages\n",
      "    Pclass  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S   Age\n",
      "5        3      0      0   8.4583         1           1           0  47.2\n",
      "17       2      0      0  13.0000         1           0           1  25.6\n",
      "19       3      0      0   7.2250         0           0           0  23.0\n",
      "Shape before imputation: (889, 8)\n",
      "Shape with imputed values: (889, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>47.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  Embarked_S\n",
       "0       3  22.0      1      0   7.2500         1           0           1\n",
       "1       1  38.0      1      0  71.2833         0           0           0\n",
       "2       3  26.0      0      0   7.9250         0           0           1\n",
       "3       1  35.0      1      0  53.1000         0           0           1\n",
       "4       3  35.0      0      0   8.0500         1           0           1\n",
       "5       3  47.2      0      0   8.4583         1           1           0\n",
       "6       1  54.0      0      0  51.8625         1           0           1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Splitting data - on whether or not \"Age\" is specified.\n",
    "\n",
    "# Training data -- \"Age\" Not null; \"Age\" as target\n",
    "train = titanic_knn[titanic_knn.Age.notnull()]\n",
    "X_train = train.drop(['Age'], axis = 1)\n",
    "y_train = train.Age\n",
    "\n",
    "\n",
    "# Data to impute, -- Where Age is null; Remove completely-null \"Age\" column.\n",
    "impute = titanic_knn[titanic_knn.Age.isnull()].drop(['Age'], axis = 1)\n",
    "print(\"Data to Impute\")\n",
    "print(impute.head(3))\n",
    "\n",
    "# import algorithm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Instantiate\n",
    "knr = KNeighborsRegressor()\n",
    "\n",
    "# Fit\n",
    "knr.fit(X_train, y_train)\n",
    "\n",
    "# Create Predictions\n",
    "imputed_ages = knr.predict(impute)\n",
    "\n",
    "# Add to Df\n",
    "impute['Age'] = imputed_ages\n",
    "print(\"\\nImputed Ages\")\n",
    "print(impute.head(3))\n",
    "\n",
    "# Re-combine data frames\n",
    "titanic_imputed = pd.concat([train, impute], sort = False, axis = 0)\n",
    "\n",
    "# Return to original order - to match back up with \"Survived\"\n",
    "titanic_imputed.sort_index(inplace = True)\n",
    "print(\"Shape before imputation:\", titanic_knn.shape)\n",
    "print(\"Shape with imputed values:\", titanic_imputed.shape)\n",
    "titanic_imputed.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Exploratory data analysis\n",
    "\n",
    "Now that we have prepared our data, we can look at the categorical features in the table above. This will be useful to tune our algorithm and decide whether our solution is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All combinations of categorical variables: \n",
      " [('Pclass', 'Sex'), ('Pclass', 'Embarked'), ('Sex', 'Embarked')] \n",
      "\n",
      "Row Percents: \n",
      " Sex       female      male\n",
      "Pclass                    \n",
      "1       0.429907  0.570093\n",
      "2       0.413043  0.586957\n",
      "3       0.293279  0.706721 \n",
      "\n",
      "Column Percents: \n",
      " Sex       female      male\n",
      "Pclass                    \n",
      "1       0.294872  0.211438\n",
      "2       0.243590  0.187175\n",
      "3       0.461538  0.601386 \n",
      "---------------\n",
      "\n",
      "Row Percents: \n",
      " Embarked         C         Q         S\n",
      "Pclass                                \n",
      "1         0.397196  0.009346  0.593458\n",
      "2         0.092391  0.016304  0.891304\n",
      "3         0.134420  0.146640  0.718941 \n",
      "\n",
      "Column Percents: \n",
      " Embarked         C         Q         S\n",
      "Pclass                                \n",
      "1         0.505952  0.025974  0.197205\n",
      "2         0.101190  0.038961  0.254658\n",
      "3         0.392857  0.935065  0.548137 \n",
      "---------------\n",
      "\n",
      "Row Percents: \n",
      " Embarked         C         Q         S\n",
      "Sex                                   \n",
      "female    0.233974  0.115385  0.650641\n",
      "male      0.164645  0.071057  0.764298 \n",
      "\n",
      "Column Percents: \n",
      " Embarked         C         Q         S\n",
      "Sex                                   \n",
      "female    0.434524  0.467532  0.315217\n",
      "male      0.565476  0.532468  0.684783 \n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Lists of categorical vs. numeric features\n",
    "categorical = ['Pclass','Sex','Embarked']\n",
    "numeric = ['Age','SibSp','Parch','Fare']\n",
    "\n",
    "# Create all the pairs of categorical variables and look at the distributions\n",
    "cat_combos = list(itertools.combinations(categorical, 2))\n",
    "print(\"All combinations of categorical variables: \\n\",cat_combos, \"\\n\")\n",
    "for row, col in cat_combos:\n",
    "    print(\"Row Percents: \\n\",pd.crosstab(titanic_df[row], titanic_df[col], normalize=\"index\"), \"\\n\")\n",
    "    print(\"Column Percents: \\n\", pd.crosstab(titanic_df[row], titanic_df[col], normalize=\"columns\"),\"\\n---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For the numeric variables, we have created a correlation heatmap using the function \"heatmap\" in the \"seaborn\" plotting package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(titanic_df[numeric].corr(), cmap = \"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q3'></a>\n",
    "\n",
    "### Question 3:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Decide whether the following statement is True or False.\n",
    "\n",
    "\"Aside from autocorrelation, two of the above variables in the heatmap have a correlation that is greater than 0.5.\" Assign your answer as a boolean value 'ans3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans3 = False\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"code\"></a>\n",
    "## Coding Logistic Regression\n",
    "\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q4'></a>\n",
    "\n",
    "### Question 4:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "The first function we will be coding will perform the pre-processing of our data. \n",
    "\n",
    "Define a function called 'prepare_data' that takes arguments as- \"numpy\" arrays \"input_x\" and \"target_y\".\n",
    "Your function should perform the following steps:\n",
    "\n",
    "1. Ensure that the 'input_x' and 'target_y' arrays have the observations as rows, and features as columns. In particular:\n",
    "    - 'input_x' should be a matrix with $n$ rows and $d$ columns. Where $n>d$\n",
    "    - 'target_y' should be a one-dimensional numpy array of length $n$.  \n",
    "\n",
    "2. A column of ones must be added to \"input_x\" matrix, increasing its dimensions to $ n \\times d+1$.  \n",
    "\n",
    "3. Ensure that \"target_y\" has all values encoded as 1 and -1, **not** 1 and 0.  \n",
    "\n",
    "4. The initial  weights must be created as a zero vector of length $d+1$ (Hint: look at the function 'np.zeros')\n",
    "\n",
    "Your function should return three arrays 'return prepared_x', 'prepared_y', 'initial_w', each created using the above steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def prepare_data(input_x, target_y):\n",
    "    \"\"\"\n",
    "    Confirm dimensions of x and y, transpose if appropriate;\n",
    "    Add column of ones to x;\n",
    "    Ensure y consists of 1's and -1's;\n",
    "    Create weights' array of all 0s\n",
    "    \n",
    "    Return X, y, and weights.\n",
    "    \n",
    "    Arguments:\n",
    "        input_x - a numpy array \n",
    "        target_y - a numpy array\n",
    "        \n",
    "    Returns:\n",
    "        prepared_x -- a 2-d numpy array; first column consists of 1's,\n",
    "            more rows than columns\n",
    "        prepared_y -- a numpy array consisting only of 1s and -1s\n",
    "        initial_w -- a 1-d numpy array consisting of \"d+1\" 0s, where\n",
    "            \"d+1\" is the number of columns in \"prepared_x\"\n",
    "        \n",
    "    Example:\n",
    "        x = np.array([[1,2,3,4],[11,12,13,14]])\n",
    "        y = np.array([1,0,1,1])\n",
    "        x,y,w = prepare_data(x,y)\n",
    "        \n",
    "        print(x) #--> array([[ 1,  1, 11],\n",
    "                            [ 1,  2, 12],\n",
    "                            [ 1,  3, 13],\n",
    "                            [ 1,  4, 14]])\n",
    "                            \n",
    "        print(y) #--> array([1, -1, 1, 1])\n",
    "        \n",
    "        print(w) #--> array([0., 0., 0.])\n",
    "        \n",
    "    Assumptions:\n",
    "        Assume that there are more observations than features in `input_x`\n",
    "    \"\"\"  \n",
    "    # Ensure shape of x-array\n",
    "    if input_x.shape[0] < input_x.shape[1]:\n",
    "        input_x = np.transpose(input_x)\n",
    "    # Check size of y array, if necessary reshape to -1\n",
    "    \n",
    "    if len(target_y.shape) > 1:\n",
    "        if min(target_y.shape) == 1:\n",
    "            target_y.reshape(-1)\n",
    "        else:\n",
    "            print(\"Bad Y\")\n",
    "        \n",
    "    # Create column of ones\n",
    "    ones = np.ones((input_x.shape[0],1), dtype = int)\n",
    "    \n",
    "    # prepend column of ones\n",
    "    augmented_x = np.concatenate((ones,input_x), axis = 1)\n",
    "    \n",
    "    # Ensure target is all -1 and 1\n",
    "    target_y = np.array([x if x ==1 else -1 for x in target_y])\n",
    "    \n",
    "    # Create initial weights of 0s\n",
    "    init_w = np.zeros(augmented_x.shape[1])\n",
    "    \n",
    "    # Return three numpy arrays\n",
    "    return augmented_x, target_y, init_w\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q5'></a>\n",
    "\n",
    "### Question 5:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Next, we will define a function to calculate the value of the sigmoid. \n",
    "\n",
    "Recall that the equation for the sigmoid is given by:\n",
    "\n",
    "$$\\sigma_i(y_i \\cdot w) = \\frac{e^{y_iX_i^Tw}}{1+e^{y_ix_i^Tw}}$$  \n",
    "\n",
    "Define a function called 'sigmoid_single' that takes the arguments- the arrays $x_i$, $y_i$, and $w$.\n",
    "\n",
    "It returns the sigmoid, as a float, with a value between 0 and 1.  \n",
    "\n",
    "**Note that $e^{y_ix_i^Tw}$ will evaluate to $np.inf$ if $y_ix_i^Tw$ is greater than ~709.782. In this case, a \"1\" should be returned by the function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "def sigmoid_single(x, y, w):\n",
    "    \"\"\"\n",
    "    Obtain the value of a Sigmoid using the training data.\n",
    "    \n",
    "    Arguments:\n",
    "        x - a vector of length d\n",
    "        y - either 1, or -1\n",
    "        w - a vector of length d\n",
    "    \n",
    "    Example:\n",
    "        x = np.array([23.0,75])\n",
    "        y = -1\n",
    "        w = np.array([2,-.5])\n",
    "        sig = sigmoid_single(x, y, w)\n",
    "        \n",
    "        print(sig) #--> 0.0002034269780552065\n",
    "        \n",
    "        x2 = np.array([ 1. , 22., 0. , 1. , 7.25 , 0. , 3. , 1. , 1.])\n",
    "        w2 = np.array([ -10.45 , -376.7215 , -0.85, -10.5 , 212.425475 , -1.1, -36.25 , -17.95 , -7.1])\n",
    "        y2 = -1\n",
    "        sig2 = sigmoid_single(x2,y2,w2)\n",
    "        \n",
    "        print(sig2) #--> 1\n",
    "    \"\"\"\n",
    "    \n",
    "    exponent = y*np.matmul(x.T,w)\n",
    "    \n",
    "    if exponent > 709.782:\n",
    "        return 1\n",
    "    else:\n",
    "        exp = np.exp(exponent)\n",
    "        \n",
    "    return exp / (1+exp)\n",
    "    \n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 05",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q6'></a>\n",
    "\n",
    "### Question 6:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "With the sigmoid, $\\sigma_i(y_i \\cdot w)$ defined above, we can define the rest of the function that is summed to calculate the gradient of the log-likelihood.   \n",
    "\n",
    "Define a function named \"to_sum\" that takes, as input, two vectors of length d, $x_i$ and $w_i$, and the paramenter $y$ equal to either 1, or -1. Your function will eventually be summed to find the gradient of the log-likelihood. \n",
    "\n",
    "Your function should return the value of $(1-\\sigma_i(y_i\\cdot w))y_ix_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "def to_sum(x,y,w):\n",
    "    \"\"\"\n",
    "    Obtain the value of the function that will eventually be summed to \n",
    "    find the gradient of the log-likelihood.\n",
    "    \n",
    "    Arguments:\n",
    "        x - a vector of length d\n",
    "        y - either 1 or -1\n",
    "        w - a vector of length d\n",
    "        \n",
    "    Example:\n",
    "        x = np.array([23.0,75])\n",
    "        y = -1\n",
    "        w = np.array([.1,-.2])\n",
    "        print(to_sum(x,y,w)) # --> array([-7.01756737e-05, -2.28833719e-04])\n",
    "    \n",
    "    \"\"\"\n",
    "    # Use function created above, multiply by x and y arrays.\n",
    "    return (1- sigmoid_single(x,y,w))*y*x\n",
    "\n",
    "    \n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q7'></a>\n",
    "\n",
    "### Question 7:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Next, code a function called 'sum_all' that will obtain and return the gradient of the log-likelihood.  \n",
    "\n",
    "Your function should take the inputs:\n",
    "- The pre-processed matrices corresponding to 'X', 'y'\n",
    "- The weights 'w'\n",
    "\n",
    "The function should return:\n",
    "$$\\sum_{i = 1}^n (1 âˆ’ \\sigma_i(y_i \\cdot w))\\ y_i x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "def sum_all(x_input, y_target, w):\n",
    "    \"\"\"\n",
    "    Obtain and return the gradient of the log-likelihood\n",
    "    \n",
    "    Arguments:\n",
    "        x_input - *preprocessed* an array of shape n-by-d\n",
    "        y_target - *preprocessed* a vector of length n\n",
    "        w - a vector of length d\n",
    "        \n",
    "    Example:\n",
    "        x = np.array([[1,22,7.25],[1,38,71.2833]])\n",
    "        y = np.array([-1,1])\n",
    "        w = np.array([.1,-.2, .5])\n",
    "        print(sum_all(x,y,w)) #--> array([-0.33737816, -7.42231958, -2.44599168])\n",
    "        \n",
    "    \"\"\"\n",
    "    # Create array of zeros for gradient\n",
    "    grad = np.zeros(len(w))\n",
    "    \n",
    "    # iteratively sum for each element in x/y\n",
    "    for x,y in zip(x_input, y_target):\n",
    "        grad += to_sum(x,y,w)\n",
    "    return grad\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q8'></a>\n",
    "\n",
    "### Question 8:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Code a function called 'update_w', that performs a single-step of gradient descent for calculating the Logistic Regression weights.\n",
    "\n",
    "Your function should take the inputs as:\n",
    "- The pre-processed arrays 'input_x' and 'target_y' \n",
    "- The current weights $w$ \n",
    "- $\\eta$, a positive float with a value close to zero\n",
    "\n",
    "The function 'update_w' should return:\n",
    "\n",
    "$$w_i + \\eta \\sum_{i = 1}^n (1 âˆ’ \\sigma_i(y_i \\cdot w_i))\\ y_i x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "def update_w(x_input, y_target, w, eta):\n",
    "    \"\"\"Obtain and return the updated Logistic Regression weights\n",
    "    \n",
    "    Arguments:\n",
    "        x_input - *preprocessed* an array of shape n-by-d\n",
    "        y_target - *preprocessed* a vector of length n\n",
    "        w - a vector of length d\n",
    "        eta - a float, positive, close to 0\n",
    "        \n",
    "    Example:\n",
    "        x = np.array([[1,22,7.25],[1,38,71.2833]])\n",
    "        y = np.array([-1,1])\n",
    "        w = np.array([.1,-.2, .5])\n",
    "        eta = .1\n",
    "        \n",
    "        print(update_w(x,y,w, eta)) #--> array([ 0.06626218, -0.94223196,  0.25540083])\n",
    "\"\"\"\n",
    "    return w + (eta * sum_all(x_input, y_target, w))\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 08",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q9'></a>\n",
    "\n",
    "### Question 9:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "\n",
    "Next, create a function called 'fixed_iteration'. It will perform the gradient descent and calculate the Logistic Regression weights for a specified number of steps.  \n",
    "\n",
    "Your function should take:\n",
    "- *Un-preprocessed* x- and y- matrices\n",
    "- An $\\eta$ parameter, a positive float close to 0 \n",
    "- 'Steps', an integer defining the number of steps of gradient descent\n",
    "\n",
    "The function 'fixed_iteration' should return the weights $w_{steps}$, calculated using an integer 'steps', where, \n",
    "\n",
    "$$w_{i+1} = w_i + \\eta \\sum_{i = 1}^n (1 âˆ’ \\sigma_i(y_i \\cdot w_i))\\ y_i x_i$$\n",
    "\n",
    "Note: Initial weights ($w_0$) should all be 0's returned from the function 'prepare_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def fixed_iteration(x_input, y_target, eta, steps):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return weights calculated from 'steps' the number of steps of gradient descent.\n",
    "    \n",
    "    Arguments:\n",
    "        x_input - *NOT-preprocessed* an array\n",
    "        y_target - *NOT-preprocessed* a vector of length n\n",
    "        eta - a float, positve, close to 0\n",
    "        steps - an int\n",
    "        \n",
    "    Example:\n",
    "        x = np.array([[22,7.25],[38,71.2833],[26,7.925],[35,53.1]])\n",
    "        y = np.array([-1,1,1,1])\n",
    "        eta = .1\n",
    "        steps = 100\n",
    "        \n",
    "        print(fixed_iteration(x,y, eta, steps)) #--> np.array([-0.9742495,  -0.41389924, 6.8199374 ])\n",
    "    \n",
    "    \"\"\"\n",
    "    # preprocess data\n",
    "    x_input, y_target, w = prepare_data(x_input, y_target)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        w = update_w(x_input, y_target, w, eta)\n",
    "    return w\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 09",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q10'></a>\n",
    "\n",
    "### Question 10:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "\n",
    "For the final function, we will create the label 'prediction' for out-of-sample data.  \n",
    "\n",
    "Code a function called 'predict'. It accepts inputs as:\n",
    "- an **un-preprocessed** numpy array 'input_x' with the observations \n",
    "- The array 'weights' that contains the weights \n",
    "\n",
    "Your function should return the label 'prediction' for 'input_x' observations; either -1 or 1 (integers).  \n",
    "\n",
    "**Hint**  First preprocess 'input_x'. Then, if `input_x`$^T\\cdot w > 0$,  predict 1. Otherwise, predict -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "def predict(x_input, weights):\n",
    "    \"\"\"\n",
    "    Return the label prediction, 1 or -1 (an integer), for the given x_input and LR weights.\n",
    "    \n",
    "    Arguments:\n",
    "        x_input - *NOT-preprocessed* a vector of length d-1\n",
    "        weights - a vector of length d\n",
    "               \n",
    "    Example:\n",
    "        Xs = np.array([[22,7.25],[38,71.2833],[26,7.925],[35,53.1]])\n",
    "        weights = np.array([0,1,-1])\n",
    "        \n",
    "        for X in Xs:\n",
    "            print(predict(X,weights))\n",
    "            #-->     1\n",
    "                    -1\n",
    "                     1\n",
    "                    -1\n",
    "    \"\"\"\n",
    "    # Add intercept term to x\n",
    "    x_input = np.insert(x_input, 0, 1)\n",
    "    \n",
    "    prod = np.matmul(x_input,weights)\n",
    "    \n",
    "    if prod > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"sklearn\"></a>\n",
    "### Logistic Regression in `sklearn`\n",
    "\n",
    "The following cells will demonstrate Logistic Regression using `sklearn`, and compare the custom Logistic Regression built in the previous functions to `sklearn's`\n",
    "\n",
    "For a more complete description of how to perfor Logistic Regression in `sklearn` you can visit [Logistic Regression in `sklearn` - Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  \n",
    "\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q11'></a>\n",
    "\n",
    "### Question 11:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Use the function `LogisticRegression` from `sklearn` to instantiate the classifier `lr`.\n",
    "\n",
    "\n",
    "Use the function `fit()` to fit `titanic_imputed` and  `y_target`  to the classifier.\n",
    "\n",
    "Finally, use the function `predict()` to create a prediction using the data `titanic_imputed`. Define this prediction to be `sk_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(titanic_imputed, y_target)\n",
    "sk_pred = lr.predict(titanic_imputed)\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 11",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.02549109]\n",
      "[[-1.10575403e+00 -3.93798289e-02 -3.21711067e-01 -6.82370169e-02\n",
      "   2.28960371e-03 -2.53809915e+00  2.53383652e-01 -2.73201193e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(lr.intercept_)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If the above functions are correctly defined, the below cell should work.\n",
    "The particular coefficients will be very different because the regularization is implemented in the 'sklearn' instantiation. However, the signs should be mostly the same. Make sure that custom version coded above has a similar result to the one given by 'sklearn'.\n",
    "\n",
    "**FOR FASTER GRADING TRY COMMENTING OUT THE BELOW CELLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # This cell may take a while\n",
    "# wt = fixed_iteration(titanic_imputed.values, y_target.values, .05, 12000)\n",
    "\n",
    "# print(wt)\n",
    "\n",
    "# cust_preds = np.array([predict(x,wt) for x in titanic_imputed.values])\n",
    "# cust_preds[cust_preds == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# print(\"sklearn:\")\n",
    "# print(classification_report(y_target, sk_pred))\n",
    "\n",
    "# print(\"Custom:\")\n",
    "# print(classification_report(y_target, cust_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
