{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Natural language Processing\n",
    "\n",
    "\n",
    "**_Author: Jessica Cervi_**\n",
    "\n",
    "**Expected time = 2 hours**\n",
    "\n",
    "**Total points = 80 points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "\n",
    "This assignment provides an overview of *Natural Language Processing* (NLP) as an approach to classification problems in supervised learning involving textual data (in particular using *Naive Bayes classifiers*). In spite of the \"naive\" assumptions involved, Naive Bayes works very well in practice particularly for text analysis in, for instance, spam filtering or document classification. As such, for this assignment, you will build a very simple model of spam filtering to get a sense of how naive Bayes classification really works. Then, you will explore the use of tools within Scikit-Learn for NLP.\n",
    "\n",
    "The primary goals of the current assignment are:\n",
    "+ to become familiar with the terminology and tools available for NLP;\n",
    "+ to practice the application of Bayes' theorem for probabilistic reasoning; and\n",
    "+ to develop a (highly simplified) model of text analysis for spam classification using the naive Bayes classification framework.\n",
    "\n",
    "\n",
    "\n",
    "This assignment is designed to build your familiarity and comfort coding in Python while also helping you review key topics from the module. As you progress through the assignment, answers will get increasingly complex. It is important that you adopt a data scientist's mindset when completing this assignment. **Remember to run your code from each cell before submitting your assignment.** Running your code beforehand will notify you of errors and give you a chance to fix your errors before submitting. You should view your Vocareum submission as if you are delivering a final project to your manager or client. \n",
    "\n",
    "***Vocareum Tips***\n",
    "- Do not add arguments or options to functions unless you are specifically asked to. This will cause an error in Vocareum.\n",
    "- Do not use a library unless you are expicitly asked to in the question. \n",
    "- You can download the Grading Report after submitting the assignment. This will include feedback and hints on incorrect questions. \n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "\n",
    "- Learn the main concepts behind the theory of Natural Language Processing\n",
    "- Represent a document-term matrix in Python\n",
    "- Learn the theory behind TD-IDF Vectorizer and its Python implementation\n",
    "- Implement Bayes Theorem in Python\n",
    "- Prearing text for analysis and distinguish between spam and ham messages\n",
    "- Computing prions and likelihoods of your prediction\n",
    "-  Using a `Scikit-Learn`  `MultinomialNB` Estimator\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Index: \n",
    "\n",
    "####   Natural language Processing\n",
    "\n",
    "- [Question 1](#q01)\n",
    "- [Question 2](#q02)\n",
    "- [Question 3](#q03)\n",
    "- [Question 4](#q04)\n",
    "- [Question 5](#q05)\n",
    "- [Question 6](#q06)\n",
    "- [Question 7](#q07)\n",
    "- [Question 8](#q08)\n",
    "- [Question 9](#q09)\n",
    "- [Question 10](#q10)\n",
    "- [Question 11](#q11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Natural language Processing\n",
    "\n",
    "**Natural Language Processing**, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language.\n",
    "The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\n",
    "\n",
    "In this assingment, we will guide your through your own implementation of an algorithm to analyze text.\n",
    "\n",
    "As usual we begin by importing the necessery libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the eventual task of classifying text messages, here is a Python string containing the body of several text messages on separate lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have a safe trip to Nigeria. Wish you happiness and very soon company to share moments with\n",
      "Well keep in mind I've only got enough gas for one more round trip barring a sudden influx of cash\n",
      "Yes i have. So that's why u texted. Pshew...missing you so much\n",
      "This school is really expensive. Have you started practicing your accent. Because its important. And have you decided if you are doing 4years of dental school or if you'll just do the nmde exam.\n",
      "Sorry, I'll call later\n",
      "Anything lor. Juz both of us lor.\n",
      "Get me out of this dump heap. My mom decided to come to lowes. BORING.\n",
      "Why don't you wait 'til at least wednesday to see if you get your .\n",
      "REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\n",
      "This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.\n",
      "Pity, * was in mood for that. So...any other suggestions?\n"
     ]
    }
   ],
   "source": [
    "messages = '''Have a safe trip to Nigeria. Wish you happiness and very soon company to share moments with\n",
    "Well keep in mind I've only got enough gas for one more round trip barring a sudden influx of cash\n",
    "Yes i have. So that's why u texted. Pshew...missing you so much\n",
    "This school is really expensive. Have you started practicing your accent. Because its important. And have you decided if you are doing 4years of dental school or if you'll just do the nmde exam.\n",
    "Sorry, I'll call later\n",
    "Anything lor. Juz both of us lor.\n",
    "Get me out of this dump heap. My mom decided to come to lowes. BORING.\n",
    "Why don't you wait 'til at least wednesday to see if you get your .\n",
    "REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\n",
    "This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.\n",
    "Pity, * was in mood for that. So...any other suggestions?'''\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The content of the preceding string illustrates a lot of the challenges with **natural language processing** from text:\n",
    "\n",
    "+ The text needs to be split into individual *tokens* (i.e., words & punctuation).\n",
    "+ There can be difficulties with upper- & lower-case interspersed with numerals and punctuation characters.\n",
    "+ Many words add little contextual information such as articles, conjunctions, etc. These are *stop words*.\n",
    "+ Similar words can occur with common roots (e.g., 'go' and 'goes', 'liked' and 'likes' and 'liked', etc.\n",
    "+ Words can be spelled incorrectly.\n",
    "\n",
    "Let's convert all the text to lower case and construct a list with the individual messages as a *corpus* of text.\n",
    "\n",
    "As a reminder, **a text corpus is a large body of text**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have a safe trip to nigeria. wish you happiness and very soon company to share moments with',\n",
       " \"well keep in mind i've only got enough gas for one more round trip barring a sudden influx of cash\",\n",
       " \"yes i have. so that's why u texted. pshew...missing you so much\",\n",
       " \"this school is really expensive. have you started practicing your accent. because its important. and have you decided if you are doing 4years of dental school or if you'll just do the nmde exam.\",\n",
       " \"sorry, i'll call later\",\n",
       " 'anything lor. juz both of us lor.',\n",
       " 'get me out of this dump heap. my mom decided to come to lowes. boring.',\n",
       " \"why don't you wait 'til at least wednesday to see if you get your .\",\n",
       " 'reminder from o2: to get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode',\n",
       " 'this is the 2nd time we have tried 2 contact u. u have won the £750 pound prize. 2 claim is easy, call 087187272008 now1! only 10p per minute. bt-national-rate.',\n",
       " 'pity, * was in mood for that. so...any other suggestions?']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = messages.lower().split('\\n')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "We'll use the following terms at various places in the assignment. More details & examples will be proveded where appropriate.\n",
    "\n",
    "+ *Stop Words* -- Specific words that are not considered important for text analysis, e.g., 'the', 'is', 'a',  etc.\n",
    "+ *Tokenization*  -- Segmentation of text into separate *tokens* (i.e., words or punctuation marks). This is a form of feature extraction.\n",
    "+ *Stemming* -- Reducing words to their root form by truncating characters, e.g., car, cars, car’s, cars’ all have *stem* 'car'.\n",
    "+ *Lemmatization* -- Grouping together the inflected forms of a word as a single item known as the *lemma* or dictionary form.\n",
    "+ *Word Embedding* -- Explicit mapping to represent sequences of tokens (words extracted from text) to vectors of real numbers.\n",
    "+ *$n$-grams* -- Sequences of words or tokens (i.e., phrases) rather than single words. Helps with better understanding of text; 'not happy' instead of 'happy,' e.g bi-gram per token. For example, the following sentence decomposes as shown into unigrams (1-grams) or bigrams (2-grams):\n",
    "\n",
    "    Sentence:\t`The movie was not great.`<br>\n",
    "    Uni-grams:\t`[‘The’, ‘movie’, ‘was’, ‘not’, ‘great.’]`<br>\n",
    "    Bi-grams:\t`[‘The movie’, ‘movie was’, ‘was not’, ‘not great.’]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Constructing a Word Embedding with the `CountVectorizer` class\n",
    "\n",
    "Scikit-Learn provides many important tools for converting textual data to numerical data (as numerical data is required for most machine learning techniques). One such tool is the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class from the module `sklearn.feature_extraction.text`. The outputs of the `transform` and `fit_transform` methods of the `CountVectorizer` class are [*document-term matrices*](https://en.wikipedia.org/wiki/Document-term_matrix) that contain word counts for each word in the corpus.\n",
    "\n",
    "As an example, here is a (modified) excerpt from the Scikit-Learn [`CountVectorizer` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object X obtained has type <class 'scipy.sparse.csr.csr_matrix'>\n",
      "The columns of X correspond to the words:\n",
      "['087187272008', '10p', '2nd', '4years', '50', '750', 'accent', 'and', 'any', 'anything', 'are', 'at', 'barring', 'because', 'boring', 'both', 'bt', 'call', 'cash', 'claim', 'come', 'company', 'contact', 'credit', 'decided', 'dental', 'details', 'do', 'doing', 'don', 'dump', 'easy', 'enough', 'exam', 'expensive', 'for', 'free', 'from', 'gas', 'get', 'got', 'great', 'happiness', 'have', 'heap', 'house', 'if', 'important', 'in', 'influx', 'is', 'its', 'just', 'juz', 'keep', 'later', 'least', 'll', 'lor', 'lowes', 'me', 'mind', 'minute', 'missing', 'mom', 'moments', 'mood', 'more', 'much', 'my', 'name', 'national', 'nigeria', 'nmde', 'no', 'now1', 'o2', 'of', 'offers', 'one', 'only', 'or', 'other', 'out', 'per', 'pity', 'pls', 'postcode', 'pound', 'pounds', 'practicing', 'prize', 'pshew', 'rate', 'really', 'reminder', 'reply', 'round', 'safe', 'school', 'see', 'share', 'so', 'soon', 'sorry', 'started', 'sudden', 'suggestions', 'text', 'texted', 'that', 'the', 'this', 'til', 'time', 'to', 'tried', 'trip', 'us', 'valid', 've', 'very', 'wait', 'was', 'we', 'wednesday', 'well', 'why', 'wish', 'with', 'won', 'yes', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(f\"The object X obtained has type {type(X)}\")\n",
    "print(f\"The columns of X correspond to the words:\\n{vectorizer.get_feature_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "After calling toarray(), the object X obtained has type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X = X.toarray() # convert to dense representation\n",
    "print(X)\n",
    "print(f\"After calling toarray(), the object X obtained has type {type(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>contact</th>\n",
       "      <th>wednesday</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    and  contact  wednesday  yes  you\n",
       "0     1        0          0    0    1\n",
       "1     0        0          0    0    0\n",
       "2     0        0          0    1    1\n",
       "3     1        0          0    0    4\n",
       "4     0        0          0    0    0\n",
       "5     0        0          0    0    0\n",
       "6     0        0          0    0    0\n",
       "7     0        0          1    0    2\n",
       "8     2        0          0    0    0\n",
       "9     0        1          0    0    0\n",
       "10    0        0          0    0    0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Represent document-term matrix using a DataFrame instead:\n",
    "X = pd.DataFrame(data=X, columns=vectorizer.get_feature_names())\n",
    "# Extract select columns\n",
    "X[['and', 'contact', 'wednesday', 'yes', 'you']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Notice a few particular properties of the `CountVectorizer` class from the preceding example.\n",
    "\n",
    "+ Each column of the document-term matrix corresponds to a particular word (as displayed by the `get_feature_names` method).\n",
    "+ By default, the text is converted to lower case (e.g., \"Wednesday\" $\\mapsto$ \"wednesday\").\n",
    "+ The specific vocabulary that determines the results of `get_feature_names` can be learned from some input text/corpus or predetermined when the object is instantiated (see documentation).\n",
    "+ Each row of the document-term matrix corresponds to a sentence from the original corpus.\n",
    "+ The numerical entries of the document-term matrix are nonnegative integers corresponding to counts of the occurrences of each word in the text corpus.\n",
    "+ The default input for the `fit` method is a list of strings or file objects corresponding to documents from which the distributions of word counts can be learned.\n",
    "+ The default output after applying the `fit_transform` method to text (or, equivalently, applying the `fit` method and subsequently applying the `transform` method to the same data) is a *sparse matrix* with only nonzero entries represented (i.e., to make storage more efficient). The purpose of calling the `toarray` method, then, is to transform the sparse matrix into a dense representation (i.e., by putting the zeros back in explicitly) for printing/display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In the next exercise, you will use the Scikit-Learn `CountVectorizer` to create a document-term matrix from slightly different text. The corpus here is modelled using a list called `text` whose entries are sentences (strings), each of which is related to the topics *TV* and *radio*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TV programs are not interesting -- TV is annoying.', 'Kids like TV', 'We receive TV by radio waves', 'It is interesting to listen to the radio', 'On the waves, kids programs are rare.', 'The kids listen to the radio; it is rare.']\n",
      "There are 6 sentences.\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "       'TV programs are not interesting -- TV is annoying.',\n",
    "       'Kids like TV'\n",
    "       ,'We receive TV by radio waves'\n",
    "       ,'It is interesting to listen to the radio'\n",
    "       ,'On the waves, kids programs are rare.'\n",
    "       ,'The kids listen to the radio; it is rare.'\n",
    "       ]\n",
    "print(text)\n",
    "print(f'There are {len(text)} sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Constructing a Document-Term Matrix\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q01'></a>\n",
    "\n",
    "\n",
    "### Question 1:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Construct a document-term matrix from the preceding list of strings `text`.\n",
    "+ Use an instance of the `CountVectorizer` class as in the preceding example.\n",
    "+ You can apply the `fit_transform` method or apply the `fit` method, and then apply the `transform` method to the data `text`.\n",
    "+ Convert the sparse array returned to a dense Numpy array.\n",
    "+ Assign the final object obtained to the identifier `transformed_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "### YOUR SOLUTION HERE\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(text)\n",
    "transformed_text = X.toarray()\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Using a `DataFrame` to Represent a Document-Term Matrix\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q02'></a>\n",
    "\n",
    "\n",
    "### Question 2:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your task here is to represent the document-term matrix `transformed_text` as a Pandas `DataFrame`. In particular, adapt the construction preceding Question 01 into the body of a function `make_dtm_df`.\n",
    "\n",
    "+ Define a function signature is `make_dtm_df(corpus)` where `corpus` is a list of strings as can be used as an input to `CountVectorizeer.fit`.\n",
    "+ The value returned is a Pandas DataFrame.\n",
    "  + The rows of the DataFrame correspond to the entries of the input corpus (i.e., there are `len(corpus)` rows).\n",
    "  + The columns of the DataFrame correspond to the words extracted using `get_feature_names` once the `CountVectorizer` is `fit` to the input `corpus`.\n",
    "  + The entries of the DataFrame are the counts of each word as they occur in the entries of `corpus` (as in the document-term matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "```python\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...    'Is this the first document?' ]\n",
    ">>> df = make_dtm_df(corpus)\n",
    ">>> df\n",
    "```\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>and</th>      <th>document</th>      <th>first</th>      <th>is</th>      <th>one</th>      <th>second</th>      <th>the</th>      <th>third</th>      <th>this</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>1</td>      <td>1</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>2</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>1</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>1</td>      <td>1</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>    </tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def make_dtm_df(corpus):\n",
    "    '''\n",
    "    This function will take in a list of and return a document-term matrix as a DataFrame.\n",
    "    INPUT:\n",
    "      corpus: list of sentences (strings)\n",
    "    OUTPUT:\n",
    "      returns DataFrame indexed by the feature names corresponding to columns of the document-term matrix.\n",
    "    '''\n",
    "    vectorizer = CountVectorizer() \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    transformed_text = X.toarray()\n",
    "    return pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "# corpus = ['This is the first document.',\n",
    "#           'This document is the second document.',\n",
    "#           'And this is the third one.',\n",
    "#           'Is this the first document?' ]\n",
    "# make_dtm_df(corpus)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "As seen previously, the DataFrame output using `text` as defined previously yields a DataFrame that can be visualized with a heatmap like this:\n",
    "\n",
    "![](./assets/table.png)\n",
    "\n",
    "As you can see, the list of features from this corpus has some words that do not contribute significant information to a topic analysis of this corpus, e.g., `'are'`, `'by'`, `'is'`, `'it'`, `'not'`, `'on'`, `'the'`, `'to'`, and `'we'`.  Such words are called *stop words* in the parlance of Natural Language Processing because they usually don't contribute much to common NLP tasks like document classification. \n",
    "\n",
    "For instance, suppose we consider the sentences in the previous list `text` as \"documents\" and each is associated with the topic `'TV'` or `'radio'`. We want to use `text` to model the topics `'TV'` and `'radio'` with the goal of classifying new documents. In that case, the words `'are'`, `'it'`, and so on from the preceding list would likely be present with similar (high) frequencies in all documents from both topics `'TV'` & `'radio'` (and hence they do not help meaningfully in distinguishing the topics).\n",
    "\n",
    "It is typical to maintain [lists of standard stop words](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html) (which vary from language to language) and to filter them out prior to building a document-term matrix. In Scikit-Learn, the `CountVectorizer` class, when instantiated, has an option `stop_words` (with default value `None`) that allows the `CountVectorizer` object to encode a corpus while omitting all occurences of stop words when counting words. The keyword argument `stop_words='english'` filters a standard built-in list of English stop words from the corpus analyzed by the `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Accepting Extra Keyword Arguments\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q03'></a>\n",
    "\n",
    "\n",
    "### Question 3:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Your task now is to modify the function `make_dtm_df` from the preceding question to allow for [*variable length keyworded arguments*](https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments).\n",
    "\n",
    "+ The function `make_dtm_df()` you'll define below  will have a single positional argument `corpus` (as before) and an additional argument [`**kwargs`](https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments).\n",
    "  + Whatever keyword arguments are passed into `make_dtm_df` should be passed into the call to `CountVectorizer` within the function body (and hence should be valid inputs for `CountVectorizer`). For instance, when calling `make_dtm_df(corpus, stop_words='english')`, the call to `CountVectorizer` inside the function body should be `CountVectorizer(stop_words='english')`.\n",
    "  + Valid values for `**kwargs` would be any keyword arguments from Scikit-Learn's [`CountVectorizer` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "+ As an example:\n",
    "\n",
    "```python\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...    'Is this the first document?' ]\n",
    ">>> make_dtm_df(corpus, stop_words='english') # Different from above...\n",
    "```\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>document</th>      <th>second</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>0</td>    </tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "def make_dtm_df(corpus, **kwargs):\n",
    "    '''\n",
    "    This function will take in a list of and return a document-term matrix as a DataFrame.\n",
    "    INPUT:\n",
    "      corpus: list of sentences (strings)\n",
    "      kwargs: any keyword arguments (e.g., stop_words=None, encoding='utf-8', etc.) `CountVectorizer` accepts\n",
    "    OUTPUT:\n",
    "      returns DataFrame indexed by the feature names corresponding to columns of the document-term matrix.\n",
    "    '''\n",
    "    vectorizer = CountVectorizer(**kwargs) \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    transformed_text = X.toarray()\n",
    "    return pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names()) \n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Introducing n-grams\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q04'></a>\n",
    "\n",
    "\n",
    "### Question 4:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Use the function `make_dtm_df` to introduce different feature encodings with [*$n$-grams*](https://en.wikipedia.org/wiki/N-gram) (i.e., sequences of $n$ consecutive words found in the input corpus).\n",
    "+ Consult the [documentation for `CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from [Scikit-Learn](https://scikit-learn.org).\n",
    "+ The required argument for `CountVectorizer` is now of the form $\\mathtt{ngram\\_range}=(p,q)$ where $p$ and $q$ are positive integers with $p<q$. This implies that the features will consist of all $k$-grams (possibly after stripping stop-words) with $p\\leq k \\leq q$. For instance, `CountVectorizer(ngram_range=(2,3))` uses all bigrams (i.e., $1$-grams) and trigrams (i.e., $3$-grams) as features.\n",
    "+ You will construct two DataFrames as the document-term matrices extracted from `text` (as given below). In particular:\n",
    "  + a DataFrame `df1` that consists of  using individual words (i.e., $1$-grams) and bigrams as features *without* filtering any stop-words.\n",
    "  + a DataFrame `df2` that consists of bigrams only *after* filtering the standard `'english'` stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame df1 has shape (6, 50)\n",
      "The DataFrame df2 has shape (6, 16)\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "###\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [\n",
    "       'TV programs are not interesting -- TV is annoying.',\n",
    "       'Kids like TV'\n",
    "       ,'We receive TV by radio waves'\n",
    "       ,'It is interesting to listen to the radio'\n",
    "       ,'On the waves, kids programs are rare.'\n",
    "       ,'The kids listen to the radio; it is rare.'\n",
    "]\n",
    "\n",
    "df1 =  make_dtm_df(text, ngram_range=(1,2))\n",
    "\n",
    "df2 = make_dtm_df(text,stop_words = 'english', ngram_range=(2, 2))\n",
    "# print(vectorizer.get_feature_names())\n",
    "### YOUR SOLUTION HERE\n",
    "# df1 = None\n",
    "# df2 = None\n",
    "    \n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "# Verification:\n",
    "print(f\"The DataFrame df1 has shape {df1.shape}\")\n",
    "print(f\"The DataFrame df2 has shape {df2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "In addition to the `CountVectorizer`, we have an option to use the [*term frequency-inverse document frequency*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) vectorization approach.  Here, rather than just pure word counts, we attempt to measure the rarity of words in the text.\n",
    "\n",
    "From the [user guide](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    ">  TF means *term-frequency* while TF–IDF means *term-frequency times inverse document-frequency*: \n",
    ">\n",
    ">  $$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t).$$\n",
    "> \n",
    ">  Using the `TfidfTransformer`’s default settings, `TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)` the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as\n",
    "> \n",
    ">  $$\\text{idf}(t) = \\log \\frac{1 + n}{1 + \\text{df}(t)} + 1$$\n",
    ">\n",
    ">  where $n$ is the total number of documents in the document set, and $\\text{df}(t)$  is the number of documents in the document set that contain term $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Introducing the TD-IDF Vectorizer\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q05'></a>\n",
    "\n",
    "\n",
    "### Question 5:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Using the function `make_dtm_df` as a model, construct a function `make_tfidf_df` that constructs a term-frequency-inverse-document-frequency matrix from a corpus of text. \n",
    "\n",
    "+ Consult the [documentation for `TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from [Scikit-Learn](https://scikit-learn.org).\n",
    "+ The function `make_tfidf_df` will have a single positional argument `corpus` (as with `make_dt_df`) and an additional argument [`**kwargs`](https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments).\n",
    "  + Whatever keyword arguments are passed into `make_tfidf_df` should be passed into the call to `TfidfVectorizer` within the function body (and hence should be valid inputs for `TfidfVectorizer`). For instance, when calling `make_tfidf_df(corpus, stop_words='english')`, the call to `TfidfVectorizer` inside the function body should be `TfidfVectorizer(stop_words='english')`.\n",
    "  + Valid values for `**kwargs` would any keyword arguments from Scikit-Learn's [`TfidfVectorizer` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "+ You will construct two DataFrames as the TFIDF matrices extracted from `text` (as given below). In particular:\n",
    "  + a DataFrame `df1` constructed *without* filtering any stop-words.\n",
    "  + a DataFrame `df2` constructed *after* filtering the standard `'english'` stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def make_tfidf_df(corpus, **kwargs):\n",
    "    '''\n",
    "    This function will take in a list of and return a TFIDF matrix as a DataFrame.\n",
    "    INPUT:\n",
    "      corpus: list of sentences (strings)\n",
    "      kwargs: any keyword arguments (e.g., stop_words=None, encoding='utf-8', etc.) `TfidfVectorizer` accepts\n",
    "    OUTPUT:\n",
    "      returns DataFrame indexed by the feature names corresponding to columns of the TFIDF matrix.\n",
    "    '''\n",
    "    vectorizer =  TfidfVectorizer(**kwargs) \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    transformed_text = X.toarray()\n",
    "    return pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 05",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Bayes's theorem\n",
    "\n",
    "Now that you've built some routines for working encoding corpuses of text as matrices (i.e., either as arrays or as DataFrames), you can build an NLP model for *spam classification*. An important part of reasoning in spam classification tasks (and related tasks using NLP) is *Bayes's theorem*:\n",
    "\n",
    "$$\\displaystyle{\\boxed{p(A\\,|\\,B) = \\frac{p(B\\,|\\,A) p(A)}{p(B)}}}.$$\n",
    "\n",
    "In the preceding, consider $A$ and $B$ as some possible *events*. With that in mind,\n",
    "+ $p(A\\,|\\,B)$ is the *posterior probability of $A$ given $B$*;\n",
    "+ $p(B\\,|\\,A)$ is the *likelihood of $B$ given $A$*;\n",
    "+ $p(A)$ is the *prior probability of $A$*; and\n",
    "+ $p(B)$ is the *evidence* (sometimes called a normalizing factor).\n",
    "\n",
    "For a more meaningful context to think about this, when sampling email or text messages, treat $A$ as the event that an incoming message is spam (i.e., undesired) and $B$ as the event that the incoming message contains the word \"bargain\". Then, what Bayes's theorem allows us to do is compute the posterior probability that an incoming message is spam given that it contains the word \"bargain\" from the corresponding likelihood of a message containing the word \"bargain\" given that it is known to be spam, from the prior probability of a message being spam, and from the probability of the word \"bargain\" appearing in any message. For convenience, let's write this as\n",
    "\n",
    "$$\\displaystyle{p(\\text{spam}\\,|\\,\\text{bargain}) = \\frac{p(\\text{bargain}\\,|\\,\\text{spam}) p(\\text{spam})}{p(\\text{bargain})}}.$$\n",
    "\n",
    "The next question requires you to apply Bayes' theorem to reason about probabilities (in the context of spam classification). Remember, the goal is to identify messages as *spam* (not wanted, undesirable) or *ham* (i.e., the opposite of spam messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Reasoning using Bayes's theorem\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q06'></a>\n",
    "\n",
    "\n",
    "### Question 06:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Assume in the following that you have a training set of 2,500 messages known to be spam and 1,250 messages known to be ham (i.e., not spam). Suppose further that the word \"holiday\" occurs in 275 of the spam messages and 13 of the ham messages.\n",
    "\n",
    "+ Assume the empirical prior probability of an incoming message being ham or spam is provided by the respective fractions of ham or spam messages in the training set.\n",
    "     + Assign the  prior probability of spam to `prior_spam` (i.e., $p(\\text{spam})$).\n",
    "     + Assign the  prior probability of ham to `prior_ham`  (i.e., $p(\\text{ham})$).\n",
    "     \n",
    "     \n",
    "+ Assume the empirical likelihood of the word \"holiday\" occurring in a message known to be spam (respectively, ham) is given by the counts above.\n",
    "     + Assign the (estimated) likelihood of \"holiday\" occurring in an incoming spam message to `likelihood_holiday_spam` (i.e., $p(\\text{holiday}\\,|\\,\\text{spam})$).\n",
    "     + Assign the (estimated) likelihood of \"holiday\" occurring in an incoming ham message to `likelihood_holiday_ham` (i.e., $p(\\text{holiday}\\,|\\,\\text{ham})$).\n",
    "     \n",
    "\n",
    "+ Finally, combine the preceding computations to estimate the *posterior* probability of an incoming message being spam given that it contains the word \"holiday\" (that is, $p(\\text{spam}\\,|\\,\\text{holiday})$).\n",
    "     + Assign the posterior probability $p(\\text{spam}\\,|\\,\\text{holiday})$ to `posterior_spam_holiday`.\n",
    "    + Assign all the values computed here to Python floating-point values up to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior_spam: 0.667\n",
      "prior_ham:  0.333\n",
      "likelihood_holiday_spam: 0.110\n",
      "likelihood_holiday_ham : 0.010\n",
      "posterior_spam_holiday: 0.077\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE:\n",
    "prior_spam = 2500/3750\n",
    "prior_ham = 1250/3750\n",
    "likelihood_holiday_spam = 275/2500\n",
    "likelihood_holiday_ham = 13/1250\n",
    "posterior_spam_holiday = (likelihood_holiday_spam*prior_spam)/(275/(275+13))\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "\n",
    "### For verifying answer:\n",
    "print('prior_spam: {:5.3f}'.format(prior_spam))\n",
    "print('prior_ham:  {:5.3f}'.format(prior_ham))\n",
    "print('likelihood_holiday_spam: {:5.3f}'.format(likelihood_holiday_spam))\n",
    "print('likelihood_holiday_ham : {:5.3f}'.format(likelihood_holiday_ham))\n",
    "print('posterior_spam_holiday: {:5.3f}'.format(posterior_spam_holiday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Filtering Spam from SMS Messages\n",
    "\n",
    "\n",
    "Now, let's introduce a particular dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), namely the [*SMS Spam Collection*]( https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). This is a famous public set of labeled SMS messages that have been collected for spam research. This is provided for you locally in the file `data/SMSSpamCollection.txt`. Here is an excerpt of various lines from `data/SMSSpamCollection.txt`:\n",
    "```\n",
    "spam    Your credits have been topped up for http://www.bubbletext.com Your renewal Pin is tgxxrz\n",
    "ham     U dun say so early hor... U c already then say...\n",
    "ham     Nah I don't think he goes to usf, he lives around here though\n",
    "ham     Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "ham     K..k:)how much does it cost?\n",
    "ham     First answer my question.\n",
    "spam    Are you unique enough? Find out from 30th August. www.areyouunique.co.uk\n",
    "ham     I'm home.\n",
    "ham     Dear, will call Tmorrow.pls accomodate.\n",
    "```\n",
    "Notice that the text is substantially messier than the toy corpuses considered above (with spelling mistakes, extra punctuation, nonstandard capitalization, and other confounding properties). The messages (or documents) are classified as `spam` (i.e., undesired messages) or as `ham` (i.e., useful, or desired messages). The labels `ham` or `spam` are separated from the rest of the text in each line by a *tab* character. The data set is also unbalanced in that there are many more `ham` messages than `spam` messages (as one would hope!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Preparing the SMS Messaging Data (I)\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q07'></a>\n",
    "\n",
    "\n",
    "### Question 7:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your next task is to load the SMS messaging data into a Pandas DataFrame `sms_messages`.\n",
    "\n",
    "+ The data is stored in a file whose location is provided for you as `FILE_PATH`.\n",
    "+ Use the function `pd.read_csv` to create the DataFrame `df` with the options `sep=\\t` (because the file is tab-separated), `header=None` (because there is no header line), and `names=['label', 'msg']`. \n",
    "+ Extract the leading column from the CSV file into a *DataFrame* `labels` (and *not* a *Series*) whose entries are either `ham` or `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "import pandas as pd\n",
    "FILE_PATH = pathlib.Path().cwd().joinpath('data', 'SMSSpamCollection.txt')\n",
    "### YOUR SOLUTION HERE\n",
    "df = pd.read_csv(FILE_PATH, sep='\\t', header=None, names=['label','msg'])\n",
    "labels = df['label']\n",
    "labels = labels.to_frame()\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "# For verifying answer:\n",
    "type(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "The text messages are noticeably messier than the toy corpuses from earlier exercises. If you apply the function `make_dtm_df` to a subset of `df['msg']`, you can see a number of meaningless tokens (\"words\") containing numbers that are extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08452810075over18</th>\n",
       "      <th>2005</th>\n",
       "      <th>21st</th>\n",
       "      <th>87121</th>\n",
       "      <th>already</th>\n",
       "      <th>amore</th>\n",
       "      <th>apply</th>\n",
       "      <th>around</th>\n",
       "      <th>available</th>\n",
       "      <th>buffet</th>\n",
       "      <th>...</th>\n",
       "      <th>tkts</th>\n",
       "      <th>to</th>\n",
       "      <th>txt</th>\n",
       "      <th>until</th>\n",
       "      <th>usf</th>\n",
       "      <th>wat</th>\n",
       "      <th>wif</th>\n",
       "      <th>win</th>\n",
       "      <th>wkly</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   08452810075over18  2005  21st  87121  already  amore  apply  around  \\\n",
       "0                  0     0     0      0        0      1      0       0   \n",
       "1                  0     0     0      0        0      0      0       0   \n",
       "2                  1     1     1      1        0      0      1       0   \n",
       "3                  0     0     0      0        1      0      0       0   \n",
       "4                  0     0     0      0        0      0      0       1   \n",
       "\n",
       "   available  buffet  ...  tkts  to  txt  until  usf  wat  wif  win  wkly  \\\n",
       "0          1       1  ...     0   0    0      1    0    1    0    0     0   \n",
       "1          0       0  ...     0   0    0      0    0    0    1    0     0   \n",
       "2          0       0  ...     1   3    1      0    0    0    0    1     1   \n",
       "3          0       0  ...     0   0    0      0    0    0    0    0     0   \n",
       "4          0       0  ...     0   1    0      0    1    0    0    0     0   \n",
       "\n",
       "   world  \n",
       "0      1  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dtm_df(df.msg.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "To clean the text up, you will remove punctuation, numbers, and tokens that begin with numbers from the messages (this does remove possibly meaningful tokens like \"2nd\" but also removes phone numbers and other distracting tokens). This is most easily achieved using the [`re`](https://docs.python.org/3/library/re.html) module for working with *regular expressions*. The essential logic is provided in the following function `process_text` (which also transforms the text to lower-case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def process_text(text):\n",
    "    new_text = re.sub('\\d+\\w*', '', text.lower())\n",
    "    for char in punctuation:\n",
    "        new_text = new_text.replace(char, '')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Preparing the SMS Messaging Data (II)\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q08'></a>\n",
    "\n",
    "\n",
    "### Question 8:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your task now is to construct a document-term matrix `sms_messages` (encoded as a DataFrame) from the `msg` column of the DataFrame `df` from the preceding exercise.\n",
    "\n",
    "+ The function `process_text` is provided for you above to strip out the offending tokens.\n",
    "+ Use the function `make_dtm_df` from before to construct the document-term matrix required. You can do this by passing the additional arguments `stop_words='english'` and using the `preprocessor=process_text` to `msg` column of the DataFrame `df`.\n",
    "+ The result should be bound to the identifier `sms_messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-0f8eb21f09b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### YOUR SOLUTION HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msms_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dtm_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m### YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-b32e778d6164>\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\d+\\w*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5135\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "sms_messages = make_dtm_df(df['msg']stop_words='english',preprocessor= process_text)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "sms_messages\n",
    "# sms_messages.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 08",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Preparing the SMS Messaging Data (III)\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q09'></a>\n",
    "\n",
    "\n",
    "### Question 09:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Finally, as usual in supervised learning, you want to split the data into training and testing data sets so that the model can be assessed after fitting it to the data.  Notice the use of the keyword argument `stratify`.\n",
    "\n",
    "+ You'll use the `train_test_split` function from the Scikit-Learn module `sklearn.model_selelction`.\n",
    "  + Use the keyword argument `random_state=13` to fix a reproducible splitting.\n",
    "  + Use the keyword argument `stratify=sms_messages['labels']` to ensure that the proportion of ham and spam messages match in the training and the testing datasets.\n",
    "  + Assign the outputs to the identifiers `sms_messages_train` & `sms_messages_test`. Notice these are both DataFrames that include both the features (i.e., the columns of the document-term matrices) and the targets/labels (i.e., ham or spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-48-52eb5780fc82>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-52eb5780fc82>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    sms_messages_train = X\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "### YOUR SOLUTION HERE\n",
    "X, y, l, p = train_test_split(sms_messages,labels, random_state=13, stratify= sms_messages['labels']\n",
    "sms_messages_train = X\n",
    "sms_messages_test = y \n",
    "labels_train = l \n",
    "labels_test = p\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "# Verification:\n",
    "print('There are {} training observations & {} testing observations.\\n'\n",
    "        .format(len(sms_messages_train), len(sms_messages_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 09",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "With the data loaded and the training & test sets prepared, we can use the training data to determine empirical estimates for the prior probabilities of messages being classified as spam or ham. Notice we want to avoid using data from the test set to compute anything to ensure that it can be used to assess the accuracy of the model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Computing Priors\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q10'></a>\n",
    "\n",
    "\n",
    "### Question 10:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Estimate prior probabilities of messages being `ham` and `spam` empirically using the training set.\n",
    "\n",
    "+ Construct a Pandas Series `priors` with Index values `'ham'` and `'spam'` and corresponding values given by the fraction of `ham` and `spam` messages (respectively) in the training set `sms_messages_train`.\n",
    "\n",
    "\n",
    "+ HINT: the Pandas Series method `value_counts` is useful here. Alternatively, you can do this using a suitable call to the `groupby` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE:\n",
    "ham = labels_train.value_counts().to_numpy()[0]\n",
    "spam = labels_train.value_counts().to_numpy()[1]\n",
    "priors = pd.series([ham,spam])\n",
    "priors.index(['ham','spam'])\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "\n",
    "### For verifying answer:\n",
    "print('Training priors (%):\\n===================\\n{}\\n'.format(100 * priors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "With the prior probabilities estimated from the training set, you need to be able to compute *likelihoods* next. Remember, given any word $w$ in the vocabulary given by the labels of the document-term matrix,\n",
    "the likelihood $p(w\\,|\\,\\text{ham})$ (and, respectively $p(w\\,|\\,\\text{spam})$) is a conditional probability of a message containing the word $w$ given that it is ham (respectively, spam). This can be worked out by iterating over the messages, counting the number of ham (respectively, spam) messages in which the word $w$ occurs and dividing by the total number of ham (respectively, spam) messages. We can write this as\n",
    "\n",
    "$$ p(w\\,|\\,C) \\simeq \\frac{|C \\cap w|}{|C|} $$\n",
    "\n",
    "where $C$ is the set of all ham (respectively, spam) messages, $C \\cap w$ is the set of messages in set $C$ that include the word $w$, and the cardinality of sets is denoted using a delimiting pair of vertical bars (e.g., $|C|$ is the number of elements in the set $C$).\n",
    "\n",
    "There is a problem, however, when a particular word in the vocabulary fails to occur in a particular subset of the training data. For instance, suppose the word *meeting* occurs in the set of ham training messages but does not in any of the *spam* training messages. In that case, the set $\\text{spam} \\cap \\text{meeting}$ is empty and the empirical likelihood of finding the word \"meeting\" in a spam message will be computed as zero. This is an artifact of the particular corpus of training messages; that is, that word does not happen to occur in any of the finite number of spam messages in the training set. Unfortunately, it is not reasonable to assume that, more generally, the likelihood of the word \"meeting\" occurring in *any* incoming spam message is zero.\n",
    "\n",
    "To compensate for this kind of problem, you can use [*Laplace smoothing*](https://en.wikipedia.org/wiki/Laplacian_smoothing). That is, modify the computation of an empirical likelihood as follows:\n",
    "\n",
    "$$ p(w\\,|\\,C) \\simeq \\frac{|C \\cap w| + \\color{red}{\\beta}}{|C| + \\color{red}{n_{\\text{w}}} } $$\n",
    "\n",
    "In the above, $C$ refers to any available categories in the classification problem (i.e., ham or spam here), $w$ is the sought event (in this case, a word $w$ occurring in a message in the set $C$), $\\beta$ is a *smoothing parameter* (typically 1) and $n_w$ is another parameter (in this case, the number of words in the vocabulary to scan for). This has the effect of augmenting likelihoods away from zero (that are problematic when multiplying likelihoods together as is required for naive Bayes classification).\n",
    "\n",
    "Your next task is to construct a function to compute likelihoods of given words in spam or ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Computing Likelihoods\n",
    "\n",
    "Next, we encapsulate the ideas above into another function that operates on DataFrames. The function `get_likelihoods` below computes the *empirical likelihoods* of a word being in a message of all possible categories (`ham` and `spam` in this case).\n",
    "\n",
    "\n",
    "+ The inputs are `words` (a list of strings), `messages` (a DataFrame), and `beta` (default 1.0).\n",
    "+ The input DataFrame `messages` is assumed to have structure as output by `make_dtm_df` from above. This is precisely the form of `sms_messages_train`.\n",
    "+ The strings in `words` can be upper or lower case. If a string in `words` is not in `messages.columns`, it will be ignored.\n",
    "+ The function returns a Pandas DataFrame:\n",
    "  + The Index contains the categories of `messages['labels']` (`ham` and `spam` in this case).\n",
    "  + The columns are given by the input list `words` (assumed to be a subset of the columns of the input `messages`).\n",
    "  + The values in the DataFrame column labeled `'word'` are computed by Laplace smoothing as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_likelihoods(words, messages, labels, beta=1):\n",
    "    n = labels.label.value_counts()\n",
    "    n_w = len(words)\n",
    "    is_spam = (labels.label == 'spam')\n",
    "    d = {}\n",
    "    # Clean up words as needed\n",
    "    new_words = pd.Index(list(map(lambda t: t.lower(), words))).intersection(sms_messages_train.columns)\n",
    "    for word in new_words:\n",
    "        count_spam = messages.loc[ is_spam, word].sum()\n",
    "        count_ham  = messages.loc[~is_spam, word].sum()\n",
    "        d[word] = pd.Series({'ham': (count_ham + beta)/(n['ham'] + n_w), 'spam': (count_spam + beta)/(n['spam'] + n_w)})\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "\n",
    "# Verification:\n",
    "words = ['customer', 'congrats', 'meeting']\n",
    "likelihoods = get_likelihoods(words, sms_messages_train, labels_train)\n",
    "print(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## The Naive Bayes's Assumption\n",
    "\n",
    "Finally, you can now try to see how to put these pieces together to classify the testing data as spam or ham. The key assumption in [*naive Bayes classification*](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) is that the *likelihoods are independent* (i.e., that the likelihood of each word occurring in a message known to be spam or ham is independent of the other words occurring). This is the \"naive\" assumption, but it makes the computations much simpler (particularly when the vocabulary is large, say, thousands of words).\n",
    "\n",
    "Under this assumption, the joint likelihood of the words $w_1$ and $w_2$ occuring in category $C$ can be determined as a product:\n",
    "\n",
    " $$ p(w_1 \\cap w_2 \\,|\\,C) = p(w_1\\,|\\,C)\\times p(w_2\\,|\\,C). $$\n",
    " \n",
    " This generalizes easily to arbitrarily many words/features (and even arbitrarily many classes in a multi-class classification problem): \n",
    " \n",
    "  $$ p(w_1, w_2, \\dotsc, w_d\\,|\\, C) = \\prod_{k=1}^{d}p(w_{k}\\,|\\,C).$$\n",
    "\n",
    "Let's see how to do this with an example message from the test data. We'll extract a single row (a count-vectorized message) and extract the columns that correspond to words that actually occurred in that message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 10                                   # Integer index of message to extract\n",
    "message = sms_messages_test.iloc[k]      # Row of document-term matrix\n",
    "y_true = labels_test.iloc[k, 0]          # Corresponding label\n",
    "words = list(message[message > 0].index) # Corresponding words in message\n",
    "print(f'After applying the count vectorizer, message {k} includes the following words: {words}.\\nThis message is known a prior to be {y_true}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation developed so far, we can compute the likelihoods of the words in `words` occurring in a ham or spam message using the function `get_likelihoods`. From there, the *joint likelihoods* of all the relevant words occurring in a message are easily computed using the Pandas DataFrame method `prod`. Observe that some words are more likely to occur in spam than in ham and some are less likely to occur in spam than in ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_likelihoods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c9a0f742adfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlikelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_likelihoods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msms_messages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlikelihoods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_likelihoods' is not defined"
     ]
    }
   ],
   "source": [
    "likelihoods = get_likelihoods(words, sms_messages_train, labels_train)\n",
    "likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'likelihoods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-41c586237ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoint_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihoods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Use Naive Bayes assumption of independence of individual likelihoods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_likelihoods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'likelihoods' is not defined"
     ]
    }
   ],
   "source": [
    "joint_likelihoods = likelihoods.prod(axis=1)    # Use Naive Bayes assumption of independence of individual likelihoods\n",
    "print(joint_likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now that the joint likelihoods of the meassage  being spam or ham (given the words) are known, they can be combined to compute the priors and finally the posterior probabilities as Bayes's theorem tells us. And when the posterior probabilities are known for both the ham and spam classes, the larger of the two can be used to decide how to classify a new message instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joint_likelihoods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8b43ad305f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjoint_likelihoods\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mposteriors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_likelihoods\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpriors\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mevidence\u001b[0m  \u001b[0;31m# Bayes' theorem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Posteriors:\\n{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposteriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposteriors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Take largest posterior probability to predict class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_true = {}\\ny_pred = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joint_likelihoods' is not defined"
     ]
    }
   ],
   "source": [
    "evidence = (joint_likelihoods * priors).sum()\n",
    "posteriors = joint_likelihoods * priors / evidence  # Bayes' theorem\n",
    "print('Posteriors:\\n{}\\n'.format(posteriors))\n",
    "y_pred = posteriors.idxmax()     # Take largest posterior probability to predict class\n",
    "print('y_true = {}\\ny_pred = {}'.format(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Computing Posteriors\n",
    "\n",
    "\n",
    "Next, we combine the ideas from the preceding lines of code into a function `get_posteriors` that returns a DataFrame with posterior probabilities of messages being ham or spam.\n",
    "\n",
    "+ The inputs are `messages` (a DataFrame), `training` (another DataFrame) and `beta` (default 1.0).\n",
    "  + The input DataFrame `messages` is similar, but it does not include a `labels` column (i.e., the targets for supervised learning).\n",
    "  + The input DataFrame `training` is assumed to have structure as output by `make_dtm_df` from above. This is precisely the form of `sms_messages_train`.\n",
    "  + The input `beta` is the parameter required for Laplace smoothing (default value 1.0).\n",
    "+ The function returns a Pandas DataFrame:\n",
    "  + The (row) `Index` is the same as the input DataFrame `messages` (i.e., the labels of the messages to classify).\n",
    "  + The `columns` attribute contains the labels `'ham'` and `'spam'`.\n",
    "  + The values are the corresponding posterior probabilities of each message being classified as ham or spam respectively\n",
    "  contains the categories of `messages['labels']` (`ham` and `spam` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posteriors(messages, training, labels, beta=1):\n",
    "    \"\"\"Computes empirical posterior probabilities for classification of rows of *messages*\n",
    "    INPUT:\n",
    "      messages:    DataFrame with document-term frequency matrix (ints)\n",
    "      training:    DataFrame with document-term frequency matrix (ints)\n",
    "      labels:      DataFrame with column 'label' (categorical)\n",
    "      beta:        (default value 1) Smoothing constant as required by get_likelihoods\n",
    "    OUTPUT:\n",
    "      posteriors:  DataFrame with same (row) index as messages['labels'] as (row) Index and training['labels']\n",
    "                   as column index. The entries are the empirical posterior probabilities of each given row\n",
    "                   being classified as the corresponding category (ham or spam in this case).\n",
    "                   Empirical likelihoods computed using Laplace smoothing\n",
    "    EXAMPLE:\n",
    "    >>> messages = sms_messages_test.iloc[:3]\n",
    "    >>> posteriors = get_posteriors(messages, sms_messages_train, labels_train)\n",
    "    >>> print(posteriors)\n",
    "                   ham      spam\n",
    "    1303  1.600012e-01  0.839999\n",
    "    4198  1.137179e-22  1.000000\n",
    "    1710  9.998436e-01  0.000156\n",
    "    >>> posteriors = get_posteriors(messages, sms_messages_train, labels_train, beta=0.5)\n",
    "    >>> print(posteriors)\n",
    "                   ham      spam\n",
    "    1303  8.702614e-01  0.129739\n",
    "    4198  1.943228e-24  1.000000\n",
    "    1710  9.999977e-01  0.000002\n",
    "    \"\"\"\n",
    "    priors = labels.label.value_counts()/len(labels)\n",
    "    posteriors = {}\n",
    "    for idx, row in messages.iterrows():\n",
    "        words = row[row>0].index\n",
    "        likelihoods = get_likelihoods(words, training, labels, beta)\n",
    "        joint_likelihoods = likelihoods.prod(axis=1)\n",
    "        evidence = (joint_likelihoods * priors).sum()\n",
    "        posteriors[idx] = joint_likelihoods * priors / evidence  # Bayes' theorem\n",
    "    return pd.DataFrame(data=posteriors).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Below, we modify the function `get_posteriors` above to yield a function `classify_messages`.\n",
    "\n",
    "\n",
    "+ The inputs are the same as for `get_posteriors`, i.e., `messages` (a DataFrame), `training` (another DataFrame) and `beta` (default 1.0).\n",
    "+ This function returns a Pandas Series:\n",
    "  + The (row) `Index` is the same as the input DataFrame `messages` (i.e., the labels of the messages to classify).\n",
    "  + The values are the labels `'ham'` and `'spam'` (according to how each message is classified).\n",
    "  + The labels are determined using whichever category has the largest corresponding posterior probability of being classified as ham or spam respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_messages(messages, training, labels, beta=1):\n",
    "    priors = labels.label.value_counts()/len(labels)\n",
    "    posteriors = {}\n",
    "    for idx, row in messages.iterrows():\n",
    "        words = row[row>0].index\n",
    "        likelihoods = get_likelihoods(words, training, labels, beta)\n",
    "        joint_likelihoods = likelihoods.prod(axis=1)\n",
    "        evidence = (joint_likelihoods * priors).sum()\n",
    "        posteriors[idx] = joint_likelihoods * priors / evidence  # Bayes' theorem\n",
    "    posteriors = pd.DataFrame(data=posteriors).T\n",
    "    preds = posteriors.apply(lambda t: t.idxmax(), axis=1)\n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "#Verification\n",
    "messages = sms_messages_test.iloc[:3]\n",
    "y_pred = classify_messages(messages, sms_messages_train, labels_train)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In practice, you don't have to implement all these probabilistic computations from scratch. The *Multinomial Naive Bayes estimator* in [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) implements a lot of this logic for you. As with other Scikit-Learn Estimator classes, there are `fit` and `predict` methods that provide the required logic.\n",
    "\n",
    "From the [Scikit-Learn documentation]((https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB):\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "As preparation for the next exerise, you will import theh `MultinomialNB` class from the module `sklearn.naive_bayes`. It's also useful to apply the DataFrame `squeeze` method to transform the single-column DataFrames `labels`, `labels_train`, and `labels_test` into Pandas Series; this is required to ensure that the targets passed into the `MultinomialNB` estimators have the appropriate shape (some Scikit-Learn estimator classes are more forgiving in accepting  distinct Pandas/NumPy compatible inputs than others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-83d52631d1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "print(type(labels), type(labels_train), type(labels_test))\n",
    "labels = labels.squeeze()\n",
    "labels_train = labels_train.squeeze()\n",
    "labels_test = labels_test.squeeze()\n",
    "print(type(labels), type(labels_train), type(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Using a Scikit-Learn `MultinomialNB` Estimator\n",
    "[Back to top](#Index:) \n",
    "<a id='q11'></a>\n",
    "\n",
    "\n",
    "### Question 11:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "\n",
    "For this exercise, you will instantiate and apply a `MultinomialNB` estimator to make spam predictions.\n",
    "\n",
    "+ The `MultinomialNB` class is imported from `sklearn.naive_bayes` for you.\n",
    "+ You will instantiate an object of the `MultinomialNB` class without keyword arguments.\n",
    "+ You will fit the model to the training data using `sms_messages_train` and `labels_train`.\n",
    "+ In addition, once the model has been fit to the training data, use the `score` method and the testing data `sms_messages_test` & `labels_test` to determine how well the model works. Assign the corresponding result to `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "nbayes = None\n",
    "accuracy = None\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "# Verification:\n",
    "print(nbayes.classes_)\n",
    "print(nbayes.class_count_)\n",
    "print(f'The accuracy of the classifier with default (alpha=1.0) smoothing is {accuracy:5.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 11",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "Not bad! You can experiment with a TFIDF matrix or a DTM with $n$-grams and with various levels of Laplacian smoothing to see to what degree various hyperparameters affect the efficacy of the classifier obtained.\n",
    "\n",
    "\n",
    "Beyond Scikit-Learn, many more convenient implementations of tools for natural language processing are available through the [NLTK](https://www.nltk.org/) (the Python [Natural Language Toolkit](https://www.nltk.org/)). The NLTK contains many useful modules for manipulating text, easy-to-use interfaces to [large corpora of text](http://www.nltk.org/nltk_data/), and an [online book for learning NLP](http://nltk.org/book)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
