{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Agents\n",
    "\n",
    "\n",
    "**_Author: Jessica Cervi_**\n",
    "\n",
    "**Expected time = 1.5 hours**\n",
    "\n",
    "**Total points = 75 points**\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "This assignment is designed to reinforce your knowledge about Logical Agents. In the first part of the assignment, you will explore and contribute to an implementation of the Wumpus world game. The Wumpus Worldâ€™s agent is an example of a knowledge-based agent that represents knowledge representation, reasoning, and planning. \n",
    "In the second part of the assignment, you will work on propositional logic. In particular, we will\n",
    "implement functions for printing truth tables for formulas with two variables. We will conclude this second part of the assignment by implementing a function to print to truth table for formulas with more than two variables.\n",
    "The last part of the assignment focuses on forward chaining.\n",
    "Forward chaining is used to track  how things change over time and to draw conclusions from a priori clauses.\n",
    "\n",
    "\n",
    "This assignment is designed to build your familiarity and comfort coding in Python while also helping you review key topics from the module. As you progress through the assignment, answers will get increasingly complex. It is important that you adopt a data scientist's mindset when completing this assignment. **Remember to run your code from each cell before submitting your assignment.** Running your code beforehand will notify you of errors and give you a chance to fix your errors before submitting. You should view your Vocareum submission as if you are delivering a final project to your manager or client. \n",
    "\n",
    "***Vocareum Tips***\n",
    "- Do not add arguments or options to functions unless you are specifically asked to. This will cause an error in Vocareum.\n",
    "- Do not use a library unless you are explicitly asked to in the question. \n",
    "- You can download the Grading Report after submitting the assignment. This will include feedback and hints on incorrect questions. \n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Implement a basic version of the Wumpus World game\n",
    "- Understand Logical Agents and their Python implementation\n",
    "- Define and visualize truth tables in Python\n",
    "- Understand the applications of forward chaining\n",
    "- Create lagged features and a time series for each feature\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Index: \n",
    "\n",
    "####  Logical Agents\n",
    "\n",
    "- [Question 1](#q01)\n",
    "- [Question 2](#q02)\n",
    "- [Question 3](#q03)\n",
    "- [Question 4](#q04)\n",
    "- [Question 5](#q05)\n",
    "- [Question 6](#q06)\n",
    "- [Question 7](#q07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logical Agents\n",
    "\n",
    "## Propositional Logic\n",
    "\n",
    "\n",
    "In this section of the assignment, you will implement functions for printing truth tables for formulas with variables. You may use the following helper function `prints`, which prints a tab-delimited list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prints(values):\n",
    "    print(\"\\t\".join([str(value) for value in values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\tFalse\tTrue\n"
     ]
    }
   ],
   "source": [
    "prints([True, False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also use the following helper function `variables`, which returns a list of the argument names of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-cff42ecb2272>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-cff42ecb2272>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def variabl:es(f)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def variabl:es(f)\n",
    "    return list(f.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "def variables (f):\n",
    "    return inspect.getargspec(f).args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x', 'y', 'z']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def h(x,y,z): return (y or x) and (not(z) <= x)\n",
    "\n",
    "variables(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to top](#Index:) \n",
    "<a id='q01'></a>\n",
    "\n",
    "\n",
    "### Question 1:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Implement a function `truthtableXY(f)` that takes as its input a single function `f` (i.e. a Python function corresponding to a logical formula). You may assume `f` takes two boolean arguments `x` and `y`. The function should print a truth table for `f`.\n",
    "\n",
    "\n",
    "If you define your function correctly, you should obtain the following output:\n",
    "```Python\n",
    "def f(x,y):\n",
    "    return x and y\n",
    "\n",
    "truthtableXY(f)\n",
    "\n",
    "y      x      formula\n",
    "True   True   True\n",
    "True   False  False\n",
    "False  True   False\n",
    "False  False  False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def truthtableXY(f):\n",
    "    prints(['y', 'x', 'formula'])\n",
    "    for x in [True, False]:\n",
    "        for y in [True, False]:\n",
    "            prints([x, y, f(x,y)])\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to top](#Index:) \n",
    "<a id='q02'></a>\n",
    "\n",
    "\n",
    "### Question 2:\n",
    "\n",
    "*20 points*\n",
    "     \n",
    "Implement a recursive function `truthtable(f)` that takes as its first argument a single function `f` (i.e. a Python function corresponding to a formula) and as a second argument `values` set to `None` by default. The function `f` may take any non-zero quantity of arguments. The function should print a truth table for `f`.\n",
    "\n",
    "\n",
    "Your `truthtable()` function should employ the recursive backtracking approach, and can be organized as follows:\n",
    "\n",
    "- The function should have a second parameter `values` with a default value of [], which will be the list of values the function builds up and eventually passes to `f`.\n",
    "- If the list `values` is empty, the function should print a row containing all the variable names (one column header per variable).\n",
    "- If the list `values` is the same length as the list of variables of `f`, the function should print a row of values containing all the values in `values`, as well as the result of applying `f` to that list of values (use the * operator to apply `f` to the list of arguments).\n",
    "- If the list `values` is shorter than the list of variables of `f`, the function should make recursive calls to truthtable(), with appropriate changes to the arguments of truthtable().\n",
    "\n",
    "Example:\n",
    "\n",
    "```Python\n",
    "def h(x,y,z): return (y or x) and (not(z) <= x)\n",
    "\n",
    "truthtable(h)\n",
    "x       y       z       formula\n",
    "True    True    True    False\n",
    "True    True    False   False\n",
    "True    False   True    False\n",
    "True    False   False   False\n",
    "False   True    True    True\n",
    "False   True    False   False\n",
    "False   False   True    False\n",
    "False   False   False   False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "def truthtable (f, values=None):\n",
    "    if values is None:\n",
    "        values = []\n",
    "\n",
    "    if values == []:\n",
    "        prints(variables(f))\n",
    "\n",
    "    if len(values) == len(variables(f)):\n",
    "        result = f(*values)\n",
    "        prints(values + [result])\n",
    "    else:\n",
    "        truthtable(f, values + [True])\n",
    "        truthtable(f, values + [False])\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to top](#Index:) \n",
    "<a id='q03'></a>\n",
    "\n",
    "\n",
    "### Question 3:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Implement a function `rows()` that takes as its first argument a single function `f` (i.e. a Python function corresponding to a formula). The function should return the number of rows in the truth table for `f`.\n",
    "\n",
    "Remember, the number of rows of a truth table is given by:\n",
    "\n",
    "$$\\text{rows} = 2^{\\text{number of variables}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE:\n",
    "def rows(f):\n",
    "    return 2 ** len(variables(h))\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x, y, z): return (y or x) and (not(z) <= x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment = [True, True, True]\n",
    "f(*assignment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(rows(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Chaining\n",
    "\n",
    "Often, analysts are interested in how things change over time. In a typical cross-sectional sample, even if you measure some variable today and then again a year from now, you will probably be sampling different people each time. To get a better handle on how things change for the same people over time, you need to be able to track them and follow up with them a year from now, and in future waves. This is **longitudinal data**.\n",
    "\n",
    "Longitudinal data is often used in economic and financial studies because it has several advantages over repeated cross-sectional data. For example, because longitudinal data measures how long events last for, it can be used to see if the same group of individuals remain unemployed during a recession, or whether different individuals are moving in and out of unemployment. This can help determine the factors that most affect unemployment.\n",
    "\n",
    "Python's `scikit-learn` module has a `TimeSeriesSplit` function that can help run Forward Chaining. However, the function makes an assumption that the entire DataFrame is to be treated as one entity, whereas, it is possible that the entire DataFrame is actually composed of groups of data (row-wise) each of which group should be treated to Forward Chaining separately. The [Financial Distress Prediction Data Set](https://www.kaggle.com/shebrahimi/financial-distress) that we will be using here is exactly this type of a data set.\n",
    "\n",
    "\n",
    "Here's a description of the columns in the dataset:\n",
    "\n",
    "\n",
    "```\n",
    "First column: Company represents sample companies\n",
    "\n",
    "Second column: Time shows different time periods the data belongs to. Time series length varies between 1 to 14 for each company.\n",
    "\n",
    "Third column: The target variable is denoted by \"Financial Distress\" if it is greater than -0.50 the company should be considered as healthy (0). Otherwise, it would be regarded as financially distressed (1).\n",
    "\n",
    "Fourth column to the last column: The features denoted by x1 to x83, are some financial and non-financial characteristics of the sampled companies. These features belong to the previous time period, which should be used to predict whether the company will be financially distressed or not (classification). Feature x80 is a categorical variable.\n",
    "\n",
    "```\n",
    "\n",
    "We begin by importing the necessary libraries that we will be using for this section of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "df_finance = pd.read_csv('./data/Financial Distress.csv', index_col=False,\n",
    "                 dtype={\n",
    "                     'Company': np.uint16,\n",
    "                     'Time': np.uint8,\n",
    "                     'Financial Distress': np.double\n",
    "                 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we perform some exploratory data analysis to better understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Time</th>\n",
       "      <th>Financial Distress</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>...</th>\n",
       "      <th>x74</th>\n",
       "      <th>x75</th>\n",
       "      <th>x76</th>\n",
       "      <th>x77</th>\n",
       "      <th>x78</th>\n",
       "      <th>x79</th>\n",
       "      <th>x80</th>\n",
       "      <th>x81</th>\n",
       "      <th>x82</th>\n",
       "      <th>x83</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010636</td>\n",
       "      <td>1.2810</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.87454</td>\n",
       "      <td>1.21640</td>\n",
       "      <td>0.060940</td>\n",
       "      <td>0.188270</td>\n",
       "      <td>0.52510</td>\n",
       "      <td>...</td>\n",
       "      <td>85.437</td>\n",
       "      <td>27.07</td>\n",
       "      <td>26.102</td>\n",
       "      <td>16.000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>22</td>\n",
       "      <td>0.060390</td>\n",
       "      <td>30</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.455970</td>\n",
       "      <td>1.2700</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.82067</td>\n",
       "      <td>1.00490</td>\n",
       "      <td>-0.014080</td>\n",
       "      <td>0.181040</td>\n",
       "      <td>0.62288</td>\n",
       "      <td>...</td>\n",
       "      <td>107.090</td>\n",
       "      <td>31.31</td>\n",
       "      <td>30.194</td>\n",
       "      <td>17.000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>22</td>\n",
       "      <td>0.010636</td>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.325390</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>-0.059379</td>\n",
       "      <td>0.92242</td>\n",
       "      <td>0.72926</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.044865</td>\n",
       "      <td>0.43292</td>\n",
       "      <td>...</td>\n",
       "      <td>120.870</td>\n",
       "      <td>36.07</td>\n",
       "      <td>35.273</td>\n",
       "      <td>17.000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.455970</td>\n",
       "      <td>32</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.566570</td>\n",
       "      <td>1.1131</td>\n",
       "      <td>-0.015229</td>\n",
       "      <td>0.85888</td>\n",
       "      <td>0.80974</td>\n",
       "      <td>0.076037</td>\n",
       "      <td>0.091033</td>\n",
       "      <td>0.67546</td>\n",
       "      <td>...</td>\n",
       "      <td>54.806</td>\n",
       "      <td>39.80</td>\n",
       "      <td>38.377</td>\n",
       "      <td>17.167</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.325390</td>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.357300</td>\n",
       "      <td>1.0623</td>\n",
       "      <td>0.107020</td>\n",
       "      <td>0.81460</td>\n",
       "      <td>0.83593</td>\n",
       "      <td>0.199960</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.74200</td>\n",
       "      <td>...</td>\n",
       "      <td>85.437</td>\n",
       "      <td>27.07</td>\n",
       "      <td>26.102</td>\n",
       "      <td>16.000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>29</td>\n",
       "      <td>1.251000</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Company  Time  Financial Distress      x1        x2       x3       x4  \\\n",
       "0        1     1            0.010636  1.2810  0.022934  0.87454  1.21640   \n",
       "1        1     2           -0.455970  1.2700  0.006454  0.82067  1.00490   \n",
       "2        1     3           -0.325390  1.0529 -0.059379  0.92242  0.72926   \n",
       "3        1     4           -0.566570  1.1131 -0.015229  0.85888  0.80974   \n",
       "4        2     1            1.357300  1.0623  0.107020  0.81460  0.83593   \n",
       "\n",
       "         x5        x6       x7  ...      x74    x75     x76     x77   x78  \\\n",
       "0  0.060940  0.188270  0.52510  ...   85.437  27.07  26.102  16.000  16.0   \n",
       "1 -0.014080  0.181040  0.62288  ...  107.090  31.31  30.194  17.000  16.0   \n",
       "2  0.020476  0.044865  0.43292  ...  120.870  36.07  35.273  17.000  15.0   \n",
       "3  0.076037  0.091033  0.67546  ...   54.806  39.80  38.377  17.167  16.0   \n",
       "4  0.199960  0.047800  0.74200  ...   85.437  27.07  26.102  16.000  16.0   \n",
       "\n",
       "   x79  x80       x81  x82  x83  \n",
       "0  0.2   22  0.060390   30   49  \n",
       "1  0.4   22  0.010636   31   50  \n",
       "2 -0.2   22 -0.455970   32   51  \n",
       "3  5.6   22 -0.325390   33   52  \n",
       "4  0.2   29  1.251000    7   27  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the number of unique companies and the number of time periods for each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique companies: 422\n",
      "Number of time periods per company:\n",
      "col_0    27644\n",
      "Company       \n",
      "1            4\n",
      "2           14\n",
      "3            1\n",
      "4           14\n",
      "5           14\n",
      "...        ...\n",
      "418          2\n",
      "419          3\n",
      "420          3\n",
      "421          6\n",
      "422          8\n",
      "\n",
      "[422 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Look at the Data\n",
    "print(\"Number of unique companies:\", df_finance.Company.unique().shape[0])  # 422 companies\n",
    "print(\"Number of time periods per company:\")\n",
    "print(pd.crosstab(df_finance.Company, df_finance.Time.sum()))  # Some companies have < 5 time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the data based on Groups per Company\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of 0       22\n",
       "1       22\n",
       "2       22\n",
       "3       22\n",
       "4       29\n",
       "        ..\n",
       "3667    37\n",
       "3668    37\n",
       "3669    37\n",
       "3670    37\n",
       "3671    37\n",
       "Name: x80, Length: 3672, dtype: int64>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_company = df_finance.groupby('Company')\n",
    "grouped_company.head()\n",
    "df_finance.x80.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges with Straightforward Time Series Split:\n",
    "\n",
    "As can be seen based on the above code output, although we have 1 common time variable (`Time`), we have multiple companies since we may have multiple rows belonging to the same Time value (For example, 1 row for Time 1 + Company 1, another row for Time 1 + Company 2 etc).\n",
    "\n",
    "This prevents us from using `sklearn.model_selection.TimeSeriesSplit`, as the assumption of the function with each row representing a data point from a unique instance of time (and the rows are arranged as per increasing value of time).\n",
    "\n",
    "We can still achieve our goal of implementing Forward Chaining by:\n",
    "\n",
    "- Split the Data Set into multiple groups - 1 group per Company\n",
    "- For each group, derive the indexes for Forward Chaining\n",
    "- Combine the list of indexes per group into 1 final index list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to top](#Index:) \n",
    "<a id='q04'></a>\n",
    "\n",
    "\n",
    "### Question 4:\n",
    "\n",
    "*5 points*\n",
    "     \n",
    "As mentioned in the Data Dictionary, one of the features is actually a categorical feature. We will therefore create Dummy columns.\n",
    "\n",
    "\n",
    "Use the function `get_dummies()` to transform the column `x80` into Dummy columns. Make sure to set the parameter `prefix` equal to `dummy`, `columns` equal to `x80` and `drop_first` equal to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "dummy_cols = pd.get_dummies(\n",
    "    df_finance[['x80']], \n",
    "     prefix='dummy',\n",
    "     columns=['x80'],\n",
    "     drop_first=True)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to top](#Index:) \n",
    "<a id='q5'></a>\n",
    "\n",
    "\n",
    "### Question 5:\n",
    "\n",
    "*10 points*\n",
    "     \n",
    "Combine `dummy_cols` back with original data set. \n",
    "     \n",
    "Make sure you exclude the column `x80` in the resulting DataFrame. Assign the result to `df_finance_dummy`.\n",
    "\n",
    "**HINT: Use the function `concat().`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "x_cols= [col for col in df_finance.columns if all([col.startswith('x'), col!='x80'])]\n",
    "df_finance_dummy = pd.concat([\n",
    "  df_finance[['Company', 'Time', 'Financial Distress']+x_cols].reset_index(drop=True),\n",
    "    pd.DataFrame(data=dummy_cols)],\n",
    "    axis=1)\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 05",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lagged Features\n",
    "\n",
    "With the above pre-processing step out of the way, we will now move on to the 2nd piece of Feature Engineering - creating lagged features (again, lagged features per group).\n",
    "\n",
    "Below, we provide a helper function `lagged_features` to create lagged features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to create lagged features\n",
    "def lagged_features(df_long, lag_features, window=2, lag_prefix='lag', lag_prefix_sep='_'):\n",
    "    \"\"\"\n",
    "    Function calculates lagged features (only for columns mentioned in lag_features)\n",
    "    based on time_feature column. The highest value of time_feature is retained as a row\n",
    "    and the lower values of time_feature are added as lagged_features\n",
    "    \n",
    "    :param df_long: Data frame (longitudinal) to create lagged features on\n",
    "    :param lag_features: A list of columns to be lagged\n",
    "    :param window: How many lags to perform (0 means no lagged feature will be produced)\n",
    "    :param lag_prefix: Prefix to name lagged columns.\n",
    "    :param lag_prefix_sep: Separator to use while naming lagged columns\n",
    "    :return: Data Frame with lagged features appended as columns\n",
    "    \"\"\"\n",
    "    if not isinstance(lag_features, list):\n",
    "        # So that while sub-setting DataFrame, we don't get a Series\n",
    "        lag_features = [lag_features]\n",
    "\n",
    "    if window <= 0:\n",
    "        return df_long\n",
    "\n",
    "    df_working = df_long[lag_features].copy()\n",
    "    df_result = df_long.copy()\n",
    "    for i in range(1, window+1):\n",
    "        df_temp = df_working.shift(i)\n",
    "        df_temp.columns = [lag_prefix + lag_prefix_sep + str(i) + lag_prefix_sep + x\n",
    "                           for x in df_temp.columns]\n",
    "        df_result = pd.concat([df_result.reset_index(drop=True),\n",
    "                               df_temp.reset_index(drop=True)],\n",
    "                               axis=1)\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the dataset into groups (based on companies) and create lagged features for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_finance_dummy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-26ea2f5217de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrouped_company\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_finance_dummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Company'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcols_to_lag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_finance_dummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#create empty DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_cross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_finance_dummy' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_company = df_finance_dummy.groupby('Company')\n",
    "cols_to_lag = [col for col in df_finance_dummy.columns if col.startswith('x')]\n",
    "\n",
    "#create empty DataFrame\n",
    "df_cross = pd.DataFrame()\n",
    "\n",
    "for name, group in grouped_company:\n",
    "    # For each group, calculate lagged features and rbind to df_cross\n",
    "    print('----------------------------------------------------')\n",
    "    print('Working on group:', name, 'with shape', group.shape)\n",
    "    df_cross = pd.concat([df_cross.reset_index(drop=True),\n",
    "                          lagged_features(group, cols_to_lag).reset_index(drop=True)],\n",
    "                         axis=0)\n",
    "    print('Shape of df_cross', df_cross.shape)\n",
    "    \n",
    "# Remove rows with NAs\n",
    "df_cross = df_cross.dropna()\n",
    "df_cross.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Splits per group\n",
    "\n",
    "Next, we will write a helper function, `ts_sample` to create time series splits for forward chaining. The function will return a list of tuples. Each tuple will contain 2 values - the train index and the test index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Time-Series sampling function to draw train-test splits\n",
    "def ts_sample(df_input, train_rows, test_rows):\n",
    "    \"\"\"\n",
    "    Function to draw specified train_rows and test_rows in time-series rolling sampling format\n",
    "    :param df_input: Input DataFrame\n",
    "    :param train_rows: Number of rows to use as training set\n",
    "    :param test_rows: Number of rows to use as test set\n",
    "    :return: List of tuples. Each tuple contains 2 lists of indexes corresponding to train and test index\n",
    "    \"\"\"\n",
    "    if df_input.shape[0] <= train_rows:\n",
    "        return [(df_input.index, pd.Index([]))]\n",
    "\n",
    "    i = 0\n",
    "    train_lower, train_upper = 0, train_rows + test_rows*i\n",
    "    test_lower, test_upper = train_upper, min(train_upper + test_rows, df_input.shape[0])\n",
    "\n",
    "    result_list = []\n",
    "    while train_upper < df_input.shape[0]:\n",
    "        result_list += [(df_input.index[train_lower:train_upper],\n",
    "                         df_input.index[test_lower:test_upper])]\n",
    "\n",
    "        # Update counter and calculate new indexes\n",
    "        i += 1\n",
    "        train_upper = train_rows + test_rows*i\n",
    "        test_lower, test_upper = train_upper, min(train_upper + test_rows, df_input.shape[0])\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ts_sample() per group\n",
    "\n",
    "The next step is to use `ts_sample` per group of the data. This will give rise to 1 list of index tuples per group.\n",
    "\n",
    "Moreover, because the number of time periods per group is not the same, the size of these lengths will also vary. Therefore, we will need a way to pad the shorter groups.\n",
    "\n",
    "\n",
    "For each group, apply function `ts_sample`. Depending on size of group, the output size of ts_sample (which is a list of (train_index, test_index)) tuples will vary. However, we want the size of each of these lists to be equal.\n",
    "\n",
    "To do that, we will augment the smaller lists by appending the last seen `train_index` and `test_index`. For example:\n",
    "```\n",
    "group 1 => [(Int64Index([1, 2, 3], dtype='int64'), (Int64Index[4, 5], dtype='int64)),\n",
    "             (Int64Index([1, 2, 3, 4, 5], dtype='int64'), (Int64Index([6], dtype='int64'))]\n",
    " group 2 => [(Int64Index([10, 11, 12], dtype='int64'), (Int64Index[13, 14], dtype='int64')),\n",
    "             (Int64Index([10, 11, 12, 13, 14), Int64Index([15, 16])),\n",
    "             (Int64Index([10, 11, 12, 13, 14, 15, 16]), Int64Index([17, 18]))]\n",
    "```\n",
    "             \n",
    " Above, group 2 has 3 folds whereas group 1 has 2. We will augment group 1 to also have 3 folds:\n",
    " ```\n",
    " group 1 => [(Int64Index([1, 2, 3], dtype='int64'), (Int64Index[4, 5], dtype='int64)),\n",
    "             (Int64Index([1, 2, 3, 4, 5], dtype='int64'), (Int64Index([6], dtype='int64')),\n",
    "             (Int64Index([1, 2, 3, 4, 5, 6]), Int64Index([]))]\n",
    "             ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cross' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7c010d3885b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrouped_company_cross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cross\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Company'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped_company_cross\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# For each group, calculate ts_sample and also store largest ts_sample output size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cross' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_company_cross = df_cross.groupby('Company')\n",
    "acc = []\n",
    "max_size = 0\n",
    "for name, group in grouped_company_cross:\n",
    "    # For each group, calculate ts_sample and also store largest ts_sample output size\n",
    "    group_res = ts_sample(group, 4, 4)\n",
    "    acc += [group_res]\n",
    "    # print('Working on name:' + str(name))\n",
    "    # print(acc)\n",
    "\n",
    "    if len(group_res) > max_size:\n",
    "        # Update the max_size that we have observed so far\n",
    "        max_size = len(group_res)\n",
    "\n",
    "        # All existing lists (apart from the one added latest)in acc need to be augmented\n",
    "        # to match the new max_size by appending the last value in those list (combining train and test)\n",
    "        for idx, list_i in enumerate(acc):\n",
    "            if len(list_i) < max_size:\n",
    "                last_train, last_test = list_i[-1][0], list_i[-1][1]\n",
    "                list_i[len(list_i):max_size] = [(last_train.union(last_test),\n",
    "                                                 pd.Index([]))] * (max_size - len(list_i))\n",
    "\n",
    "                acc[idx] = list_i\n",
    "\n",
    "    elif len(group_res) < max_size:\n",
    "        # Only the last appended list (group_res) needs to be augmented\n",
    "        last_train, last_test = acc[-1][-1][0], acc[-1][-1][1]\n",
    "        acc[-1] = acc[-1] + [(last_train.union(last_test), pd.Index([]))] * (max_size - len(acc[-1]))\n",
    "\n",
    "\n",
    "print(acc[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`acc` now contains a list of lists, where each internal list contains tuples of `train_index`, `test_index`:\n",
    "```\n",
    "[[(group_1_train_index1, group_1_test_index1), (group_1_train_index2, group_1_test_index2)],\n",
    "  [(group_2_train_index1, group_2_test_index1), (group_2_train_index2, group_2_test_index2)],\n",
    "  [(group_3_train_index1, group_3_test_index1), (group_3_train_index2, group_3_test_index2)]]\n",
    "```\n",
    "\n",
    "Our goal is to drill-down by removing group-divisions:\n",
    "```\n",
    "[(train_index1, test_index1), (train_index2, test_index2)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b99bfcad831f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mflat_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_acc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mflat_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "flat_acc = []\n",
    "for idx, list_i in enumerate(acc):\n",
    "    if len(flat_acc) == 0:\n",
    "        flat_acc += list_i\n",
    "        continue\n",
    "\n",
    "    for inner_idx, tuple_i in enumerate(list_i):\n",
    "        flat_acc[inner_idx] = (flat_acc[inner_idx][0].union(tuple_i[0]),\n",
    "                               flat_acc[inner_idx][1].union(tuple_i[1]))\n",
    "\n",
    "\n",
    "print(flat_acc[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Now that we have our lagged features as well as the indexes ready for Forward Chaining, we can proceed with modeling.\n",
    "\n",
    "However, one decision that we will need to take into account is whether we want to treat this as a classification problem or a regression problem. The `Financial Distress` column is real-valued, containing both positive and negative values. As per the Data Dictionary, we should consider the company financially distressed if the 'Financial Distress' column is <= -0.50. Accordingly, we will convert this problem into a classification problem by using that definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to top](#Index:) \n",
    "<a id='q06'></a>\n",
    "\n",
    "\n",
    "### Question 06:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Create a copy of the DataFrame `df_cross()`. Name this new DataFrame `df_model.`  \n",
    "\n",
    "Modify the column `Financial Distress` in `df_model` so that it contains zeros if the value is greater than -0.5 or 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cross' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-40086d1384e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### YOUR SOLUTION HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cross\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Financial Distress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.50\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Financial Distress'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cross' is not defined"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "df_model = df_cross.copy()\n",
    "df_model['Financial Distress'] = [0 if x > -0.50 else 1 for x in df_model['Financial Distress'].values]\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to top](#Index:) \n",
    "<a id='q07'></a>\n",
    "\n",
    "\n",
    "### Question 07:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Assign all the columns, except ` Financial Distress` of `df_model` to the variable `dependent_cols`. \n",
    "Assign  the column ` Financial Distress` of `df_model` to the variable `independent_col`. \n",
    "\n",
    "Use a for loop on `flat_acc` to generate the `X_train`, `X_test`, `y_train` and `y_test` sets.\n",
    "\n",
    "**HINT: Here is the code to generate X_train set. The others are similar**\n",
    "\n",
    "```Python\n",
    "X_train = df_model.loc[tuple_i[0]][dependent_cols]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b6b88d383a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### YOUR SOLUTION HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdependent_cols\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Financial Distress'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mindependent_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Financial Distress'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# for x in flat_acc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_model' is not defined"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "dependent_cols  = [col for col in df_model.columns if col != 'Financial Distress']\n",
    "independent_col = [col for col in df_model.columns if col == 'Financial Distress']\n",
    "independent_col = [col for col in df_model.columns if col == 'Financial Distress'] \n",
    "X_train = df_model.loc[tuple_i[0]][dependent_cols]\n",
    "X_test = df_model.loc[tuple_i[0]][dependent_cols]\n",
    "y_train = \n",
    "y_test = df_model.loc[tuple_i[1]][independent_col]\n",
    "# ###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each entry in `flat_acc`, perform train and test using logistic regression and print the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, tuple_i in enumerate(flat_acc):\n",
    "# Fit logistic regression model to train data and test on test data\n",
    "    lr_mod = LogisticRegression(C=0.01, penalty='l2')  # These should be determined by nested cv\n",
    "    lr_mod.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = lr_mod.predict_proba(X_test)\n",
    "    y_pred = lr_mod.predict(X_test)\n",
    "    \n",
    "    # Print Confusion Matrix and ROC AUC score\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('ROC AUC score:')\n",
    "    print(roc_auc_score(y_test['Financial Distress'].astype(int), y_pred_proba[:, 1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
