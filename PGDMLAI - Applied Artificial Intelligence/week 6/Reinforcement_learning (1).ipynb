{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "#  Introduction to Reinforcement Learning\n",
    "\n",
    "**_Author: Jacob Koehler, Guilherme Freitas and Dhavide Aruliah_**\n",
    "\n",
    "**Reviewer: Jessica Cervi**\n",
    "\n",
    "**Expected time = 2.5 hours**\n",
    "\n",
    "**Total points = 65 points**\n",
    "\n",
    "    \n",
    "## Assignment Overview\n",
    "\n",
    "\n",
    "The focus of this assignment is **Reinforcement Learning**, i.e., the branch of machine learning concerned with choosing actions in an environment where labeled input/output pairs may not be available and sub-optimal actions may not be explicitly corrected. The principal idea is to balance the exploration of uncharted territory with exploitation of current knowledge to maximize some kind of a cumulative reward. In this assignment, we aim to encapsulate the ideas from the lectures in code. In the first part, we'll use some basic pre-made environments from OpenAI's `gym` package to explore the effect of different policies on agents. In the second part, we'll experiment with a *multi-armed bandit*.\n",
    "\n",
    "This assignment is designed to build your familiarity and comfort coding in Python while also helping you review key topics from each module. As you progress through the assignment, answers will get increasingly complex. It is important that you adopt a data scientist's mindset when completing this assignment. **Remember to run your code from each cell before submitting your assignment.** Running your code beforehand will notify you of errors and give you a chance to fix your errors before submitting. You should view your Vocareum submission as if you are delivering a final project to your manager or client. \n",
    "\n",
    "***Vocareum Tips***\n",
    "- Do not add arguments or options to functions unless you are specifically asked to. This will cause an error in Vocareum.\n",
    "- Do not use a library unless you are expicitly asked to in the question. \n",
    "- You can download the Grading Report after submitting the assignment. This will include feedback and hints on incorrect questions. \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Differentiate between Reinforcerment Learning and Machine Learning\n",
    "- Implement various policies for different games\n",
    "- Represent Markov decision processes\n",
    "- Define transition probability and reward matrices\n",
    "- Implement a Q-Value iteration and extract the optimal policy\n",
    "- Implement multi-armed bandit problems in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## Index: \n",
    "\n",
    "####  Introduction to Reinforcement Learning\n",
    "\n",
    "- [Question 1](#q01)\n",
    "- [Question 2](#q02)\n",
    "- [Question 3](#q03)\n",
    "- [Question 4](#q04)\n",
    "- [Question 5](#q05)\n",
    "- [Question 6](#q06)\n",
    "- [Question 7](#q07)\n",
    "- [Question 8](#q08)\n",
    "- [Question 9](#q09)\n",
    "- [Question 10](#q10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction to Reinforcement Learning\n",
    "\n",
    "Before we begin, it is important to know the difference between Reinforcement Learning and Machine Learning.\n",
    "\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q01'></a>\n",
    "\n",
    "\n",
    "### Question 1:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Which of the following choices are characteristics of reinforcement learning problems? \n",
    "Select all that apply and construct your solutions as a list of strings\n",
    "\n",
    "+ a) There are known labels on all observations\n",
    "+ b) Data is sequential \n",
    "+ c) We are working with iid data\n",
    "+ d) Delayed feedback after actions\n",
    "\n",
    "Provide your answer as a list of strings bound to `ans_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### \n",
    "### YOUR SOLUTION HERE:\n",
    "ans_1 = ['b', 'd']\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Example\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/BlackJack6.jpg/500px-BlackJack6.jpg)\n",
    "\n",
    "The example below demonstrates a basic Reinforcement Learning problem using **OpenAI**'s gym.  Here, we replicate a game of blackjack where our actions are either to take a hit or stay.  The rules of blackjack are simple:\n",
    "\n",
    "\n",
    "- Get 21 points on the player's first two cards (called a \"blackjack\" or \"natural\"), without a dealer blackjack;\n",
    "- Reach a final score higher than the dealer without exceeding 21; or\n",
    "- Let the dealer draw additional cards until their hand exceeds 21 (\"busted\").\n",
    "\n",
    "Here, we will start a new game and explore some different strategies for when to take a card, when to hold, and when to hit.  Below is a description of the environment from the documentation [here](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py).\n",
    "\n",
    "```\n",
    "Blackjack is a card game where the goal is to obtain cards that sum to as near as possible to 21 without going over. They're playing against a fixed dealer. \n",
    "Face cards (Jack, Queen, King) have a point value of 10. \n",
    "Aces can either count as 11 or 1, and it's called 'usable' at 11. \n",
    "This game is placed with an infinite deck (or with replacement). \n",
    "The game starts with each (player and dealer) having one face up and one face down card. The player can request additional cards (hit=1) until they decide to stop (stick=0), or exceed 21 (bust).\n",
    "After the player sticks, the dealer reveals their face-down card, and draws until their sum is 17 or greater.  \n",
    "If the dealer goes bust the player wins.\n",
    "If neither player nor dealer busts, the outcome (win, lose, draw) is decided by whose sum is closer to 21. \n",
    "The reward for winning is +1, drawing is 0, and losing is -1.\n",
    "The observation of a 3-tuple of: the players current sum,the d ealer's one showing card (1-10 where 1 is ace),\n",
    "and whether or not the player holds a usable ace (0 or 1).\n",
    "```\n",
    "\n",
    "\n",
    "We begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0') # Create a new game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 6, False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a new game. tuple returned is of form\n",
    "# (player hand, dealer hand, usable ace)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 6, False), -1.0, True, {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take another card (Hit)\n",
    "# Now values are given as\n",
    "# ((player hand, dealer hand, usable ace), reward, game over, info)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 6, False), 0.0, True, {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hold \n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "### Implementing a \"Hit below 18\" Policy in Blackjack\n",
    "\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q02'></a>\n",
    "\n",
    "\n",
    "### Question 2:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Now, we construct our first policy for the **Blackjack** environment. We will take action 1 if our hand is below 18, otherwise stay. We define a function that will take in an observation tuple `(player hand, dealer hand, usable ace)` and return 1 or 0 depending on the player hand.\n",
    "\n",
    "Complete the function `hit_under_18()` below according to instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED \n",
    "### \n",
    "def hit_under_18(obs):\n",
    "    ''' \n",
    "    This function takes in an observation from the enviornment\n",
    "    and hits whenever a hand is under 18.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs: Tuple of the form (player hand, dealer hand, usable ace)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1 or 0 depending on whether player hand is > 18\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    obs1 = (14, 10, False)\n",
    "    hit_under_18(obs1) ====> 1\n",
    "    \n",
    "    obs = (19, 8, True) \n",
    "    hit_under_18(obs2) ====> 0\n",
    "    '''\n",
    "    if obs[0]>18:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "    return action\n",
    "obs1 = (19, 10, False)\n",
    "hit_under_18(obs1)\n",
    "# ###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "###  Simulating Numerous Blackjack Hands\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q03'></a>\n",
    "\n",
    "\n",
    "### Question 3:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Now that we have a policy, we will explore how it performs over a number of games. We need to construct a function `simulate_play()` that accepts:\n",
    "- `n_plays`: Number of iterations to simulate play based on.\n",
    "- `policy` : Policy function to implement decisions\n",
    "\n",
    "and returns the average results of simulating that many play iterations.  \n",
    "Your function should loop over the given number of iterations, check the status of the game and implement our `hit_under_18` policy. We want to pursue each strategy for 10 plays in each game to assure we come to completion.\n",
    "\n",
    "\n",
    "Complete the function `simulate_play()` below according to instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### \n",
    "def simulate_play(n_plays, policy):\n",
    "    '''\n",
    "    This function takes in a number of plays and a policy.\n",
    "    We return the average reward based on the plays.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    plays: Number of iterations to simulate play based on.\n",
    "    policy: Policy function to implement decisions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    avg: average reward over episodic play\n",
    "    \n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    n_plays = 500\n",
    "    policy = hit_under_18\n",
    "    print(simulate_play(n_plays, hit_under_18)) =====> \n",
    "    '''\n",
    "    average_reward = []\n",
    "    env.reset()\n",
    "    for player in range(n_plays):\n",
    "        i = 1\n",
    "        total_reward = 0 \n",
    "        #print(\"partida:\",player,\"========================\")\n",
    "        \n",
    "        while i <= 10:\n",
    "            action = policy(env._get_obs())\n",
    "            #print(\"action=\",action,\"env._get_obs()=\",env._get_obs())\n",
    "          \n",
    "            obs, reward, status, caca = env.step(action)    \n",
    "            \n",
    "            total_reward += reward        \n",
    "            if status:    \n",
    "                break\n",
    "            i += 1\n",
    "            \n",
    "        env.reset()   \n",
    "        average_reward.append(total_reward)    \n",
    "\n",
    "    return(sum(average_reward)/n_plays) \n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Representing a Markov Decision Process\n",
    "\n",
    "Now, we move to a different problem.  We aim to represent the key components of the *Markov Decision Process* as discussed from Videos 21-3 through 21-10. In particular, we'll discuss the example of a robot moving as described in Video 21-5.\n",
    "\n",
    "<center>\n",
    "    <img src=assets/MDP.png width=60% />\n",
    "</center>\n",
    "\n",
    "In the preceding image, we've adjusted our diagram to represent each individual action $a_0(\\text{move slow}), a_1(\\text{move fast})$ and their respective transition probabilities between states.\n",
    "\n",
    "We will use the image to represent this problem with three mathematical objects:\n",
    "\n",
    "- *P*: a *transition probability matrix* of the form $P(s, a, s')$.\n",
    "- *R*: a *reward matrix* of the form $R(s, a, s')$.\n",
    "- *$\\gamma$*:  a *discount rate*\n",
    "\n",
    "Technically, both $P$ and $R$ are described as three-dimensional arrays rather than \"matrices\" (two-dimensional arrays). Your next two tasks are to represent the transition probability & reward matrices as NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Defining a Transition Probability Matrix\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q04'></a>\n",
    "\n",
    "\n",
    "### Question 4:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Our task here is to extract a representation for the transition probability matrix $P$ from the preceding diagram.\n",
    "\n",
    "+ The identifier `P` should be a three-dimensional NumPy array of shape `(3, 2, 3)` (corresponding to $|S|\\times |A|\\times |S|$ where $S$ is the set of all possible states and $A$ is the set of all possible actions.\n",
    "+ The entry $P(s, a, s')$ is the probability of transition from state $s$ to state $s'$ upon executing action $a$.\n",
    "+ Assume the set of states $S$ is ordered as\n",
    "  $$ \\text{Standing}\\rightarrow0, \\text{Fallen}\\rightarrow1, \\text{Moving}\\rightarrow2.  $$\n",
    "+ Assume the set of actions $A$ is ordered as\n",
    "  $$ \\text{Slow}\\rightarrow0, \\text{Fast}\\rightarrow1. $$\n",
    "+ You can read the transition probabilities from the diagram above (or from slide 9 of the notes). In both diagrams, the transition probabilities are the leftmost numbers attached to the arrows representing transitions between states (upon appropriate actions).\n",
    "+ From the fallen state, the \"Fast\" action is not available. Rather than assigning those transition probabilities a zero, use `np.nan` to represent all transition probabilities with an initial state \"Fallen\" (i.e., $s=1$).\n",
    "\n",
    "Assign the resulting transition probability matrix as a `Numpy` array to `P`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "import numpy as np\n",
    "### YOUR SOLUTION HERE:\n",
    "P = np.array([[0.6, 0.4, 0],\n",
    "[0, np.nan, 1],\n",
    "[0.2, 0, 0.8]])\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "###  Defining a Reward Matrix\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q05'></a>\n",
    "\n",
    "\n",
    "### Question 5:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Following a logic similar to the preceding task, we now want to extract a representation for the reward matrix $R$ from the diagram.\n",
    "\n",
    "+ The identifier `R` should be a three-dimensional NumPy array of shape `(3, 2, 3)` (corresponding to $|S|\\times |A|\\times |S|$ where $S$ is the set of all possible states and $A$ is the set of all possible actions).\n",
    "+ The entry $R(s, a, s')$ is the reward accumulated by successfully transitioning from state $s$ to state $s'$ upon executing action $a$.\n",
    "+ As in the previous question, assume the set of states $S$ is ordered as\n",
    "  $$ \\text{Standing}\\rightarrow0, \\text{Fallen}\\rightarrow1, \\text{Moving}\\rightarrow2.  $$\n",
    "+ As in the previous question, assume the set of actions $A$ is ordered as\n",
    "  $$ \\text{Slow}\\rightarrow0, \\text{Fast}\\rightarrow1. $$\n",
    "+ You can read the rewards from the diagram above (or from slide 9 of the notes). In both diagrams, the rewards are the rightmost numbers attached to the arrows representing transitions between states (upon appropriate actions).\n",
    "+ From the fallen state, the \"Fast\" action is not available. Rather than assigning those transition probabilities as zero, use `np.nan` to represent all rewards with an initial state \"Fallen\" (i.e., $s=1$).\n",
    "\n",
    "Assign the resulting reward matrix as a `NumPy` array to `R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE:\n",
    "R = None\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 05",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Representing Possible Actions\n",
    "\n",
    "<center>\n",
    "    <img src=assets/MDP.png width=60% />\n",
    "</center>\n",
    "\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q06'></a>\n",
    "\n",
    "\n",
    "### Question 6:\n",
    "\n",
    "*5 points* \n",
    "\n",
    "\n",
    "Use the diagram above to determine a list of possible actions from each state.\n",
    "\n",
    "+ The desired result should be a Python object bound to `possible_actions`.\n",
    "+ `possible_actions` should be a list of length 3 (corresponding the possible states).\n",
    "+ As in the previous questions, assume the set of states $S$ is ordered as\n",
    "  $$ \\text{Standing}\\rightarrow0, \\text{Fallen}\\rightarrow1, \\text{Moving}\\rightarrow2.  $$\n",
    "+ The $k$th element of the list `possible_actions` should be lists of length 0, 1, or 2 according to which a subset of $A$ can be executed from state $k$.\n",
    "+ Remember, as in the previous questions, assume the set of actions $A$ is ordered as\n",
    "  $$ \\text{Slow}\\rightarrow0, \\text{Fast}\\rightarrow1. $$\n",
    "  Thus, these lists can only be `[]`, `[0]`, `[1]`, or `[0,1]` which mean, respectively that no action, only action \"Slow\", only action \"Fast\", and either action \"Slow\" or \"Fast\" can be executed from that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE:\n",
    "possible_actions = [[0,1],[0],[0,1]] \n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value Iteration\n",
    "\n",
    "We saw in Video 21-9 the idea of value iteration and the associated *Q-Value Iteration Algorithm*. Here, we keep track of how well our policy performs through the following formula:\n",
    "\n",
    "$$Q_{k + 1}(s, a) \\leftarrow \\sum_{s'} P(s, a, s')[ R(s, a, s') + \\gamma \\max_{a'} Q_k(s', a')] \\quad \\forall (s, a)$$\n",
    "\n",
    "To implement this algorithm, we need representations of $P$ and $R$ in advance as well as a way to represent a $Q$-table.\n",
    "For each iteration $k$, the matrix $Q_{k}$ will be a matrix of shape $S \\times A$ (at $k=0$, the value of $Q_{0}(s,a)$ is zero if action $a$ is permissible from state $s$, a.k.a is `possible_actions` is defined, and $-\\infty$ otherwise. The $Q$-value iteration starts from the $Q$-table $Q_{0}$ and subsequently implements the algorithm to update $Q_{k}(s, a)$ for $k>0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Constructing a Q-Table\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q07'></a>\n",
    "\n",
    "\n",
    "### Question 7:\n",
    "\n",
    "*5 points* \n",
    "\n",
    "As stated above, let's create our initial $Q$-table $Q_{0}$: \n",
    "\n",
    "  $$ Q_{0}=  \\begin{bmatrix} Q_{0}(0,0) & Q_{0}(0,1) \\\\ Q_{0}(1,0) & Q_{0}(1,1) \\\\ Q_{0}(2,0) & Q_{0}(2,1) \\end{bmatrix}$$\n",
    "\n",
    "+ The first argument $s$ of $Q_{0}(s,a)$ corresponds to one of the *states* in $S$.\n",
    "+ The second argument $a$ of $Q_{0}(s,a)$ corresponds to one of the *actions* in $A$.\n",
    "+ As described above,\n",
    "\n",
    "$$ Q_{0}(s,a) = \\begin{cases} 0, & \\text{if action }a\\text{ is permissible from state }s; \\\\ -\\infty &\\text{otherwise.} \\end{cases} $$\n",
    "\n",
    "+ Use `-np.inf` to represent $-\\infty$ where required.\n",
    "+ Bind the appropriate array to the identifier `Q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "Q = None\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "###  Implementing Q-Value Iteration\n",
    "\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q08'></a>\n",
    "\n",
    "\n",
    "### Question 8:\n",
    "\n",
    "*10 points* \n",
    "\n",
    "Now we are ready to implement the algorithm repeated below.  \n",
    "\n",
    "$$Q_{k + 1}(s, a) \\leftarrow \\sum_{s'} P(s, a, s')[ R(s, a, s') + \\gamma \\max_{a'} Q_k(s', a')] \\quad \\forall (s, a)$$\n",
    "\n",
    "+ To guide your implementation of the preceding formula, use the function signature for the `q_value_iteration` provided below.\n",
    "Your function takes the following arguments:\n",
    "- Q: Q-value table initialized with zero values for possible actions and -np.inf for impossible actions\n",
    "- P: Probability transition matrix\n",
    "- R: Reward matrix\n",
    "- possible_actions: List of possible actions \n",
    "- discount_rate: gamma in q-value iteration formula representing discounting of past episodes\n",
    "- n: number of iterations to run iterations\n",
    "\n",
    "Your function should return the updated Q-table after n-iterations according to the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-85159785f476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mq_value_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m### YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Q' is not defined"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "## Complete the q_value_iteration function below.\n",
    "def q_value_iteration(Q, P, R, possible_actions, discount_rate, n_iter):\n",
    "\n",
    "\n",
    "    '''\n",
    "    This function implements a Q-value iteration and returns\n",
    "    an updated Q-table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q: Q-value table initialized with zero values for possible\n",
    "    actions and -np.inf for impossible actions\n",
    "    \n",
    "    P: Probability transition matrix\n",
    "    \n",
    "    R: Reward matrix\n",
    "    \n",
    "    possible_actions: List of possible actions \n",
    "    \n",
    "    discount_rate: gamma in q-value iteration formula representing \n",
    "    discounting of past episodes\n",
    "    \n",
    "    n: number of iterations to run iterations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Q: Updated Q-table after n-iterations\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    q_value_iteration(Q, P, R, possible_actions, 0.9, 1) ---> \n",
    "    array([[ 1. ,  0.8],\n",
    "       [-0.2,  nan],\n",
    "       [ 1. ,  1.4]])\n",
    "       \n",
    "       \n",
    "    q_value_iteration(Q, P, R, possible_actions, 0.1, 1000) --->\n",
    "    '''\n",
    "    n_s, n_a = np.shape(Q)\n",
    "    for i in range (n_iter):\n",
    "        for s in range(n_s):\n",
    "            for a in possible_actions[s]:\n",
    "                temp = 0\n",
    "                for sp in range(n_s):\n",
    "                    temp += P[s][a][sp]*(R[s][a][sp]+discount_rate*max(Q[s]))\n",
    "                Q[s][a] = temp\n",
    "    return Q\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 08",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "###  Extracting the Optimal Policy\n",
    "[Back to top](#Index:) \n",
    "<a id='q09'></a>\n",
    "\n",
    "\n",
    "### Question 9:\n",
    "\n",
    "*5 points* \n",
    "\n",
    "Now that we have executed a $Q$-value iteration, we can extract an optimal policy by examining our final $Q$-table.\n",
    "\n",
    "\n",
    "Define a function `extract_policy`, that takes, as input, a Q-table and returns a one-dimensional policy with optimal action for each state.\n",
    "Note that the policy will be a one-dimensional array of length $3$ whose entries are either $0$ or $1$ (corresponding to the actions \"Slow\" or \"Fast\" respectively).\n",
    "\n",
    "**HINT: Given the final $Q$-table, we can extract a policy by determining the action $a$ that maximizes the value $Q(s,a)$ along the row corresponding to state $s$. The function `argmax()` will help you in this task**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def extract_policy(q_table):\n",
    "    '''\n",
    "    This function takes in a q-value table\n",
    "    and extracts the optimal action for each state.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    q_table: State x Action matrix of q-values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    policy: one-dimensional policy with optimal action for each \n",
    "    state\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    table_1 = q_value_iteration(Q, P, R, possible_actions, 0.9, 5000)\n",
    "    table_2 = q_value_iteration(Q, P, R, possible_actions, 0.001, 5000)\n",
    "    print(extract_policy(table_1)) ===> array([0, 0, 0])\n",
    "    print(extract_policy(table_2)) ===> array([0, 0, 1])\n",
    "    '''\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 09",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Multi-Armed Bandit Problems\n",
    "\n",
    "When you go to restaurant many times, how do you balance your desire to stick to your \"favorite dish\" with the desire to find out more about other items in the menu? That is an example of what we call a [_multi-armed bandit problem_](https://en.wikipedia.org/wiki/Multi-armed_bandit).\n",
    "\n",
    "In a _multi-armed bandit problem_ the decision-maker has to repeatedly choose among $n$ options (\"arms\") where each arm yields a random _reward_ that is drawn from a fixed but unknown probability distribution associated with that arm. In the restaurant example, each item in the menu is an arm, and the rewards are not all that random (dishes can vary, as well as your appreciation for them, but typically not much). \n",
    "\n",
    "The crucial feature of multi-armed bandit problems is the _trade-off between exploration and exploitation_: to learn what arms are better than others, we need to try a variety of them; but we need to balance that need for learning with the desire to reap the rewards of what we learned so far by using more often the arms we believe are more effective.\n",
    "\n",
    "Bandit algorithms can be especially useful because they are _online algorithms_, and can help the solution you choose stay \"good\" even if \"the world changes\", that is, the quality/rewards distribution of the arms change over time. In other words, bandit algorithms can perform well even when the reward distributions are _nonstationary_.\n",
    "\n",
    "Multi-armed bandit applications abound. For example:\n",
    "\n",
    "- Optimal ad choice for an advertising campaign\n",
    "- Optimal portfolio selection in finance\n",
    "- Optimal design of clinical trials (https://arxiv.org/pdf/1507.08025.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Standard boilerplate for Python for data science\n",
    "%matplotlib inline\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Additional imports we will use\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Fixed Policies\n",
    "\n",
    "For us, as far as implementation goes, ***a bandit is a list of functions (arms)***. This narrows down our modeling choices in a useful way. When we pick an arm to pull, we call the corresponding function, which gives us a random reward. We do not know what those functions are, but we do know that, *for each arm, there exists a fixed probability distribution such that the rewards of that arm are identically-independent distributed (iid) draws from that distribution*.\n",
    "\n",
    "For concreteness, think of each arm as a different ad that will be placed in front of a user. Each ad induces a certain fraction of the audience to click on the ad. Clicking on any one of the ads leads the user to the same landing page, and thus has the same value for the advertiser. Our goal is to pick a policy for choosing ads which leads to the highest probability of a click (click-through rate, or CTR), as soon as possible.\n",
    "\n",
    "We can thus model the arms as Bernoulli distributions with differing parameters $p \\in [0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using scipy.stats will allow us to easily generalize this beyond bernoulli rewards later\n",
    "bernoulli = sp.stats.bernoulli\n",
    "\n",
    "def make_bernoulli_bandit(probs):\n",
    "    \"\"\"Create a Multi-Armed Bandit with Bernoulli Rewards\"\"\"\n",
    "    arms = [bernoulli(p).rvs for p in probs]\n",
    "    return arms\n",
    "\n",
    "# Two different ads, one with 10% CTR, the other with 20%; much higher than in practice!\n",
    "# The CTR is not known in advance; this is for simulation purposes only\n",
    "b = make_bernoulli_bandit([.1, .2])\n",
    "\n",
    "b[0](size=100).mean(), b[1](size=100).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "As usual, we model policies $\\pi$ as maps from the state space $S$ to the action space $A$.\n",
    "\n",
    "However, for coding purposes, it will be more useful to work with transition functions (which we label $\\tau$, or \"tau\") that _map the state space into itself_, via the policy function:\n",
    "\n",
    "- Given a starting state, the policy function specifies the action.\n",
    "- The action is the choice of which bandit arm to pull.\n",
    "- Pulling the arm yields a random reward at which point we can determine the next state.\n",
    "\n",
    "Choosing which variables go into the state and how they get updated from one state to the next is an _important modeling decision_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Fixed Action\n",
    "\n",
    "We will explore various policies in the following exercises following more or less the same blueprint. Let's see that blueprint in the simplest policy possible: a fixed policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_fixed_transition(bandit, arm_id, s1=None):\n",
    "    \"\"\"Pick a given arm and pull that one forever.\n",
    "    \n",
    "    Even though there is no state to track for this, we will use a minimal\n",
    "    state variable to report on the outcomes in a similar way to other policies to come.\n",
    "    \"\"\"\n",
    "    n_arms = len(bandit)\n",
    "    s1 = s1 or {\n",
    "        't': -1,\n",
    "        'arm_rewards': np.zeros(n_arms, dtype=np.int),\n",
    "        'arm_pulls': np.zeros(n_arms, dtype=np.int),\n",
    "    }\n",
    "        \n",
    "    def tau(s=None):\n",
    "        s = deepcopy(s) or deepcopy(s1)\n",
    "        arm = bandit[arm_id]\n",
    "        reward = arm()\n",
    "        s['arm_pulls'][arm_id] += 1\n",
    "        s['arm_rewards'][arm_id] += reward\n",
    "        s['t'] += 1\n",
    "        return s\n",
    "    \n",
    "    return tau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will now define two convenience functions to simulate outcomes of a policy, one to simulate the outcomes, the other to compare the rewards in the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_bandit(tau, n_periods, random_seed=None, s=None):\n",
    "    \"\"\"Returns accumulated rewards and arm pulls of a given policy for a given bandit problem\n",
    "    \"\"\"\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "    history = list()\n",
    "    s = tau(s)\n",
    "    history.append(s)\n",
    "    for _ in range(n_periods-1):\n",
    "        s = tau(s)\n",
    "        history.append(s)\n",
    "    sim_df = pd.DataFrame.from_records(history).set_index('t')\n",
    "    return sim_df\n",
    "\n",
    "bernoulli_bandit1 = make_bernoulli_bandit([.1, .2])\n",
    "always_first_arm = make_fixed_transition(bandit=bernoulli_bandit1, arm_id=0)\n",
    "sim_df = simulate_bandit(always_first_arm, 1000)\n",
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_arms_in_simulation(sim_df, normalize=True):\n",
    "    \"\"\"Plot the rewards path over time of each arm in a given simulation\n",
    "    \n",
    "    If normalize=False, the accumulated rewards will be shown.\n",
    "    Otherwise, the running average rewards will be displayed.\n",
    "    \"\"\"   \n",
    "    rewards = pd.DataFrame.from_dict({i: col for (i, col) in enumerate(zip(*sim_df['arm_rewards']))})\n",
    "    pulls = pd.DataFrame.from_dict({i: col for (i, col) in enumerate(zip(*sim_df['arm_pulls']))})\n",
    "    if normalize:\n",
    "        perf = rewards / pulls\n",
    "    else:\n",
    "        perf = rewards\n",
    "    perf.index.name = 't'\n",
    "    perf.columns.name = 'arm_performance'\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_arms_in_simulation(sim_df).plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Note that the average performance of the first arm converges to the parameter $p$ of the corresponding Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Fixed Exploration + Greedy\n",
    "\n",
    "\n",
    "\n",
    "A fixed action policy is a crapshoot; if you are lucky, you select a good action, but if you are not lucky, then you can be stuck with a bad one. One way to reduce the chance of being stuck with a bad action is to explore for a few rounds before commiting to a fixed action. That is what we will implement below.\n",
    "\n",
    "Below, we implement the transition function tau inside make_greedy_transition that sample each arm the same number of times, and select the one with the largest accumulated rewards (same as highest average rewards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_greedy_transition(bandit, samples_per_arm, s1=None):\n",
    "    \"\"\"Draw a fixed amount of samples per arm then go with the best arm.\n",
    "    \n",
    "    The state is a means to\n",
    "    \n",
    "    - know when the transition from the exploration phase ends and when the exploitation phase begins.\n",
    "    - know which arm to pull once the exploration phase ends\n",
    "    \n",
    "    So we need to track the accumulated rewards of each arm.\n",
    "    \n",
    "    In a nutshell:\n",
    "    \n",
    "        cycle the arms until all have been pulled `samples_per_turn` times, updating accumulated rewards for each arm\n",
    "        play the best arm forever\n",
    "        \n",
    "    The initial state is given by s.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> np.random.seed(10)\n",
    "    >>> bandit = make_bernoulli_bandit([.3, .8])\n",
    "    >>> greedy = make_greedy_transition(bandit, samples_per_arm=1)\n",
    "    >>> s2 = greedy(s=None)\n",
    "    >>> s2\n",
    "    {'t': 1,\n",
    "     'arm_rewards': array([1, 0]),\n",
    "     'arm_pulls': array([1, 0]),\n",
    "     'n_exploration_pulls': 2,\n",
    "     'n_arms': 2,\n",
    "     'best_arm': None}\n",
    "    >>> s3 = greedy(s=s2)\n",
    "    >>> s3\n",
    "    {'t': 2,\n",
    "     'arm_rewards': array([1, 1]),\n",
    "     'arm_pulls': array([1, 1]),\n",
    "     'n_exploration_pulls': 2,\n",
    "     'n_arms': 2,\n",
    "     'best_arm': None}\n",
    "    >>> s4 = greedy(s=s3)\n",
    "    >>> s4\n",
    "    {'t': 3,\n",
    "     'arm_rewards': array([1, 1]),\n",
    "     'arm_pulls': array([2, 1]),\n",
    "     'n_exploration_pulls': 2,\n",
    "     'n_arms': 2,\n",
    "     'best_arm': 0}\n",
    "    \"\"\"\n",
    "    n_arms = len(bandit)\n",
    "    s1 = s1 or {\n",
    "        't': 0,\n",
    "        'arm_rewards': np.zeros(n_arms, dtype=np.float),\n",
    "        'arm_pulls': np.zeros(n_arms, dtype=np.int),\n",
    "        'n_exploration_pulls': n_arms * samples_per_arm,\n",
    "        'n_arms': n_arms,\n",
    "        'best_arm': None\n",
    "    }\n",
    "        \n",
    "    def tau(s=None):\n",
    "        s = deepcopy(s) or deepcopy(s1)\n",
    "        if s['t'] < s['n_exploration_pulls']:\n",
    "            arm_id = s['t'] % s['n_arms']  # cycle through the arms in order\n",
    "        else:\n",
    "            if s['t'] == s['n_exploration_pulls']:\n",
    "                s['best_arm'] = np.argmax(s['arm_rewards'])  # find the best arm\n",
    "            arm_id = s['best_arm']  # play the best arm forever\n",
    "        # draw the reward and update the state\n",
    "        arm = bandit[arm_id]\n",
    "        reward = arm()\n",
    "        s['arm_pulls'][arm_id] += 1\n",
    "        s['arm_rewards'][arm_id] += reward\n",
    "        s['t'] += 1\n",
    "        return s\n",
    "    \n",
    "    return tau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can now execute the cell below to simulate multiple runs of a greedy algorithm. Notice the best arm wins most of the time, but not always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_periods=100\n",
    "\n",
    "bernoulli_bandit1 = make_bernoulli_bandit([.1, .2])\n",
    "greedy = make_greedy_transition(bernoulli_bandit1, samples_per_arm=5)\n",
    "sim_df = simulate_bandit(greedy, n_periods=n_periods)\n",
    "compare_arms_in_simulation(sim_df).plot.line();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Implementing an $\\varepsilon$-Greedy Algorithm\n",
    "\n",
    "\n",
    "The problem with the greedy algorithm is that it cannot recover from a mistake; if it picks a bad arm at the end of the exploration phase, then we are stuck with that forever.\n",
    "\n",
    "A more robust way to proceed would be to always keep exploring a bit. A simple way to do that is as follows:\n",
    "\n",
    "- Explore for a fixed number of rounds\n",
    "- For every period thereafter:\n",
    "    - with probability (1-epsilon), choose the arm with the best average performance so far\n",
    "    - with probability epsilon, pick an arm at random\n",
    "\n",
    "Average performance of an arm here is just the current accumulated rewards from that arm divided by the number of pulls of that arm.\n",
    "\n",
    "In the code cell below, we implement the pseudo-code above in the pi function inside make_epsilon_greedy_transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_transition(bandit, samples_per_arm, eps, s1=None):\n",
    "    \"\"\"Implement the epsilon-greedy bandit algorithm\"\"\"\n",
    "    n_arms = len(bandit)\n",
    "    s1 = s1 or {\n",
    "        't': 0,\n",
    "        'arm_rewards': np.zeros(n_arms, dtype=np.int),\n",
    "        'arm_pulls': np.zeros(n_arms, dtype=np.int),\n",
    "        'n_exploration_pulls': n_arms * samples_per_arm,\n",
    "        'n_arms': n_arms,\n",
    "        'eps': eps\n",
    "    }\n",
    "        \n",
    "    def pi(s=None):\n",
    "        s = deepcopy(s) or deepcopy(s1)\n",
    "        if s['t'] < s['n_exploration_pulls']:\n",
    "            arm_id = s['t'] % s['n_arms']  # cycle through the arms in order\n",
    "        else:\n",
    "            if np.random.random() < s['eps']:\n",
    "                arm_id = np.random.randint(s['n_arms'])\n",
    "            else:\n",
    "                arm_id = np.argmax(s['arm_rewards'] / s['arm_pulls'])  # find the best arm\n",
    "        # draw the reward and update the state\n",
    "        arm = bandit[arm_id]\n",
    "        reward = arm()\n",
    "        s['arm_pulls'][arm_id] += 1\n",
    "        s['arm_rewards'][arm_id] += reward\n",
    "        s['t'] += 1\n",
    "        return s\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Execute the cell below multiple times to get the accumulated rewards of different arms under this policy.\n",
    "\n",
    "Notice that with `n_periods = 100` you often times end up with a bad arm, but with `n_periods = 1000`, that is virtually impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_periods = 100\n",
    "\n",
    "bernoulli_bandit1 = make_bernoulli_bandit([.1, .2])\n",
    "epsgreedy = make_epsilon_greedy_transition(bernoulli_bandit1, samples_per_arm=5, eps=.2)\n",
    "sim_df = simulate_bandit(epsgreedy, n_periods)\n",
    "compare_arms_in_simulation(sim_df).plot.line();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Implementing an Upper Confidence Bound Bandit Algorithm\n",
    "\n",
    "\n",
    "The $\\varepsilon$-greedy policy emphasizes the importance of continue to explore even after you think you have found a good or optimal solution.\n",
    "\n",
    "But we can do better than that. In the $\\varepsilon$-greedy policy when deciding to \"explore\"  that is, to learn more about the reward probabilities  we do not take into account how many times we have pulled any given arm, which is the main statistic to determine _how_ uncertain we are about the probability of reward from a given arm.\n",
    "\n",
    "Intuitively, it makes sense to be more certain about the reward probability of an arm that we pulled more times vs. an arm that we pulled fewer times. So, all else equal:\n",
    "\n",
    "- We should pull arms that have been pulled less often to increase our knowledge of the reward probabilities (exploration)\n",
    "- We should pull arms that have a higher reward probability to reap rewards of the knowledge we have (exploitation)\n",
    "\n",
    "One principled way to do that is to use a family of algorithms known as _Upper Confindence Bound_ (UCB) algorithms. The UCB1 algorithm uses Hoeffding's Inequality to provide guaranteed probability bounds on the error of the estimated rewards. The algorithm works as follows:\n",
    "\n",
    "- Start by pulling each arm exactly once at $t=1$.\n",
    "- Let $N_{t}(a)$ be the number of times arm $a$ was pulled by period $t$ and $Q_{t}(a)$ be the average reward of arm $a$ at period $t$.\n",
    "- At every period $t > 1$, pull the arm that maximizes the expression\n",
    "\n",
    "$$Q_{t}(a) + \\sqrt{\\frac{2 \\ln(t)}{N_{t}(a)}}$$\n",
    "\n",
    "This policy captures the notion of \"optimism under uncertainty\".\n",
    "\n",
    "It's not hard to see that, because $Q_{t}(a)$ is bounded, every arm will be pulled an infinite number of times, if we keep playing indefinitely.\n",
    "\n",
    "\n",
    "Below, we implement UCB1 in the pi function inside make_ucb1_transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ucb1_transition(bandit, s1=None):\n",
    "    \"\"\"Implement the UCB1 bandit algorithm\n",
    "    \"\"\"\n",
    "    n_arms = len(bandit)\n",
    "    s1 = s1 or {\n",
    "        't': 0,\n",
    "        'arm_rewards': np.zeros(n_arms, dtype=np.float),\n",
    "        'arm_pulls': np.zeros(n_arms, dtype=np.int),\n",
    "        'n_arms': n_arms,\n",
    "    }\n",
    "        \n",
    "    def pi(s=None):\n",
    "        s = deepcopy(s) or deepcopy(s1)\n",
    "        if s['t'] < s['n_arms']:\n",
    "            arm_id = s['t'] % s['n_arms']  # cycle through the arms in order\n",
    "        else:\n",
    "            avg_rewards = s['arm_rewards'] / s['arm_pulls']\n",
    "            uncertainty_term = np.sqrt(2 * np.log(1+s['t']) / s['arm_pulls'])\n",
    "            arm_id = np.argmax(avg_rewards + uncertainty_term)\n",
    "        # draw the reward and update the state\n",
    "        arm = bandit[arm_id]\n",
    "        reward = arm()\n",
    "        s['arm_pulls'][arm_id] += 1\n",
    "        s['arm_rewards'][arm_id] += reward\n",
    "        s['t'] += 1\n",
    "        return s\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Execute the cell below multiple times to get the accumulated rewards of different arms under this policy.\n",
    "\n",
    "Notice again that with `n_periods = 100` you sometimes end up with a bad arm, but with `n_periods = 1000`, that is virtually impossible. You should be able to notice that UCB1 selects the \"bad arm\" as the best less often than the epsilon-greedy policy just by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_periods = 100\n",
    "\n",
    "bernoulli_bandit1 = make_bernoulli_bandit([.1, .2])\n",
    "ucb1 = make_ucb1_transition(bernoulli_bandit1)\n",
    "sim_df = simulate_bandit(ucb1, n_periods)\n",
    "compare_arms_in_simulation(sim_df).plot.line();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Beyond Binary Rewards\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q10'></a>\n",
    "\n",
    "\n",
    "### Question 10:\n",
    "\n",
    "*10 points* \n",
    "\n",
    "So far we've had binary rewards, leading us to model our bandits as lists of Bernoulli random variables.\n",
    "\n",
    "We will now show that modeling bandits as lists of random variables was a convenient choice. We will switch from Bernoulli rewards to beta rewards with parameters $a$ and $b$ and verify we are still able to use all our transition, simulation, and plotting functions.\n",
    "\n",
    "It is desirable to write our functions/classes in a way that makes their reuse possible in a wide variety of situations. That happens when the components of our codebase (functions, data structures, etc.) are _loosely coupled_. For example, the simulation code doesn't need to know details of which policy is being used; it just needs to know how to ask a given transition function for the next state. Crucially, some state variables have to exist (like `arm_pulls`, and `arm_rewards`) across all policies/transitions for that work. Again, carefully modeling state is important for both mathematical and software engineering reasons! \n",
    "\n",
    "Below, implement the `make_beta_bandit` function according to the paragraph above. You may want to read the implementation of `make_bernoulli_bandit` earlier on. Note that `scipy.stats.bernoulli` took a single parameter `p`, and `scipy.stats.beta` takes two parameters `a` and `b` from the underlying gamma distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "###YOUR SOLUTION HERE\n",
    "\n",
    "beta = sp.stats.beta  # use this in make_beta_bandit!\n",
    "\n",
    "def make_beta_bandit(a, b):\n",
    "    \"\"\"Make a bandit with beta rewards with params alpha, beta given by arrays a, b.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    a: array with the \"alpha\" parameters of the distributions of each arm.\n",
    "    \n",
    "    b: array with the \"alpha\" parameters of the distributions of each arm. Same size as `a`\n",
    "    \"\"\"\n",
    "    return\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can now, for example, simulate what UCB1 would do for a beta bandit with 3 arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_periods = 100\n",
    "\n",
    "beta_bandit = make_beta_bandit([1, 2, 3], [1, 4, 9])\n",
    "ucb1 = make_ucb1_transition(beta_bandit)\n",
    "sim_df = simulate_bandit(ucb1, n_periods)\n",
    "compare_arms_in_simulation(sim_df).plot.line();"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "jupytext": {
   "cell_metadata_json": true,
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
